\documentclass[11pt,a4paper]{article}
\usepackage{times,latexsym}
\usepackage{url}
\usepackage[T1]{fontenc}


\usepackage{CJKutf8}
\usepackage{fullpage}

\usepackage{tikz-dependency}

\newcommand\BibTeX{B{\sc ib}\TeX}
\newcommand\confname{EMNLP-IJCNLP 2019}
\newcommand\conforg{SIGDAT}


\usepackage{amsmath}
\usepackage{tikz-dependency}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator{\E}{\mathop{\mathbb{E}}}

\usepackage{amssymb}% http://ctan.org/pkg/amssymb
\usepackage{pifont}% http://ctan.org/pkg/pifont
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%


\newcommand{\Prob}{\mathbb{P}}%

%\usepackage{pslatex}
%\usepackage{latexsym}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{bm}
\usepackage{graphicx}
\usepackage{tikz}
\usepackage{xcolor}
\usepackage{url}
%\usepackage[colorinlistoftodos]{todonotes}
\usepackage{rotating}
\usepackage{multirow}



\newcommand{\japanese}[1]{\begin{CJK}{UTF8}{min}#1\end{CJK}}


\usepackage[T1]{fontenc}

\usepackage{pslatex}
%\usepackage{latexsym}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{bm}
\usepackage{graphicx}
\usepackage{tikz}
\usepackage{xcolor}
\usepackage{url}
%\usepackage[colorinlistoftodos]{todonotes}
\usepackage{rotating}
%\usepackage{natbib}
\usepackage{amssymb}



\usepackage{natbib}
\bibliographystyle{unsrtnat}

\usepackage{amsthm}
 

\allowdisplaybreaks

\newcounter{theorem}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{question}[theorem]{Question}
\newtheorem{example}[theorem]{Example}
\newtheorem{defin}[theorem]{Definition}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{thm}[theorem]{Theorem}


\newcommand{\R}[0]{\mathbb{R}}
\newcommand{\Ff}[0]{\mathcal{F}}
%\newcommand{}[1]{\textbf{#1}}


\newcommand{\soft}[1]{}
\newcommand{\nopreview}[1]{}
\newcommand\comment[1]{{\color{red}#1}}
\newcommand\mhahn[1]{{\color{red}(#1)}}

\newcommand{\thetad}[0]{{\theta_d}}
\newcommand{\thetal}[0]{{\theta_{LM}}}
\newcommand{\thetap}[0]{{\theta_{P}}}


\title{Coadaptation between usage and grammar in the evolution of word order}

\author{Michael Hahn and Yang Xu}

\begin{document}
\maketitle


\begin{abstract}
Languages vary considerably in syntactic structure.
About 40\% of the world's languages have subject-verb-object order, and about 40\% have subject-object-verb order.
A wide range of existing work has argued that the structure of human language reflects adaptation toward efficiency in communication.
In these studies, variation across languages can reflect different optima or points along a Pareto frontier.
We report evidence for an additional functional pressure in the evolution of word order across languages:
Coadaptation between usage and grammar.
We show across 72 languages from 29 families that they exhibit the basic word order that is most efficient given the sentences typically produced by their speakers, under the joint efficiency consideration of grammar for Dependency Length Minimization.
Drawing on phylogenetic modeling and historical text data from nine languages, we show further that historical word order change over time is accompanied by change in the usage distributions.
We identify relevant characteristics that reflect this joint optimization, in particular the frequency with which subjects and objects are expressed together for the same verb.
Our findings highlight that functional optimization in language structure and in language usage go hand in hand in shaping the evolution of syntactic structure across languages.
\end{abstract}

{\color{blue}References}
This work can be a useful reference (e.g., style of writing, and standard of figures and display items): {\it Cultural influences on word meanings revealed through large-scale semantic alignment}. This work too: {\it The evolution of language families is shaped by the environment beyond neutral drift}. Both papers were published at the same venue, which is suitable for us too if you agree.\\

%{\color{blue}Suggestions and modifications of the outline}\\

%{\color{blue}1. Problem motivation and significance - cross-linguistic variation in syntactic structure, and how it is central to understanding the nature of human language (e.g. Greenberg)}\\

%{\color{blue}2. Existing theory of efficient communication}
%-Introduce efficiency-based explanations

%-Raise variation between languages and what efficiency-based theories have to say about it\\

%{\color{blue}3. Relation of efficient communication and basic word order}

% \cite{greenberg-universals-1963} 


Human languages show both tremendous variation and striking similarities in their grammatical structure. Understanding these is central to the understanding of the nature of human language.
A large body of research argues that similarities across languages are due to convergent evolution favoring efficient communication under cognitive resource limitations (CITE).
Here, we present evidence that evolution favoring efficiency jointly affects the grammatical structure of a language and the way the language is used.
%\comment{Try to insert 1 sentence here that says what we study here that differs from or advances the theory of efficient communication, at the end of the opening paragrah.}

A key dimension that languages vary along is word order.
Typologists have long classified languages according to their basic word order, the order in which they order verbs, subjects, and objects.
About 40\% of the world's languages are classified as having subject-verb-object order ({SVO}, as in English, a dog bites a human), and 40 \% are classified as having subject-object-verb order ({SOV}, dog-human-bites) (CITE).
Other orders, such as verb-subject-object (bites-dog-human) are less common.
A large fraction of languages allows different orderings, though typically with one of the possible orderings being most common or least marked, which is considered its basic word order in the typological literature.

Different accounts have been proposed for this variation.
The low frequency of object-initial orderings (e.g., OSV) arguably has satisfactory explanations (see below), but there is no consensus as to the distribution of SVO and SOV.
Some work has argued that SOV is the more default order in the history of language (CITE), and that SVO has emerged later to reduce ambiguity \citep{gibson-noisy-channel-2013}.
On the other hand, phylogenetic modeling suggests that languages can cycle between these two orders in their development \citep{maurits2014tracing}.
Efficiency-based accounts have been argued to either favor SVO or SOV \citep{maurits-why-2010, gibson-noisy-channel-2013, ferrer-i-cancho-placement-2017}.

In this work, we suggest a new explanation for the distribution of SOV and SVO order:
{Coadaptation} between grammar and usage.
We argue that neither SVO nor SOV are more efficient in principle.
Rather, languages tend to have the basic word order that is most communicatively efficient given the utterances speakers typically produce.
As languages change, usage and grammar coevolve; change in usage goes hand-in-hand with change in basic word order.
\comment{is there any existing literature or reference that we can use to ground the theory of coadaptation? or can we say in a sentence or two WHY or what motivates the coadaptation behavior---we don't want to sound like we just made this up, after we've seen what's in the data. rather we want to motivate this theory with a clear rationale, or ground it in existing proposals (if any).}
Efficiency-based accounts of language structure often model optimization for usage distributions that are held constant across languages (CITE).
Variation between languages is accounted for as reflecting different optima or points along a Pareto frontier \citep{zaslavsky2018efficient, zaslavsky2019evolution}.
However, in reality, usage patterns do differ between languages, and this might impact which grammatical patterns are more efficient for a given language.
For instance, \cite{gibson2017color} suggest that color naming systems differ between industrialized and non-industrialized societies because of differences in the usefulness of color in a society.
We therefore hypothesize that grammatical structure and usage patterns may interact in the evolution of language:
As a language changes, change in grammatical structure should be accompanied by change in the way the language is used, i.e., there is a process of {coadaptation} between grammar and use.

In the domain of word order, prominent efficiency-based proposals are centered around various locality principles, which assert that elements are ordered closer together when they are more strongly related in terms of their meaning and function \citep{behaghel1932deutsche,givon1985iconicity,rijkhoff-word-1986,hawkins-performance-1994}.
A prominent formalization of this idea is {Dependency Length Minimization} (DLM), the observation that languages tend to order words in such a way as to reduce the overall distance between syntactically related words \citep{rijkhoff-word-1986,hawkins-performance-1994,futrell-cross-linguistic-2015, liu-dependency-2017}.
This is supported by corpus studies on dozens of languages, and explains word order universals \citep{rijkhoff-word-1986, hawkins-performance-1994, hahn2020universals}.
Dependency Length Minimization and related locality principles can be justified in terms of memory efficiency \citep{gibson-linguistic-1998} and general communicative efficiency \citep{hahn2020universals}.
%Other efficiency principles include Uniform Information Density \cite{...} and noise robustness in transmission \cite{gibson-noisy-channel-2013}.

In this paper, we argue that languages have the basic word orders that are most efficient for DLM, given the utterances typically produced in a language.
To understand how DLM might make predictions about basic word order, we begin with a thought experiment.
We first consider a simple transitive sentence such as `dogs bite people' (Figure~\ref{fig:sent-dep}A). 
DLM is defined formally in terms of Dependency Grammar (CITE).
In this formalism, the syntactic structure of a sentence is drawn with directed arcs linking words -- called heads -- to those words that are syntactically subordinate to them -- called dependents.
For instance, arcs link the verb to its subjects and objects.
The length of an arc is one plus the number of other words that it crosses.
The dependency length of an entire sentence is the sum of the lengths of all dependency arcs.

In a simple sentence as in Figure~\ref{fig:sent-dep}, SVO order results in overall lower dependency length:
In SVO order, both dependencies have length $1$, resulting in a total length of $2$.
In SOV order, the arc between the verb and its subject is longer because it crosses the object, resulting in a total length of $3$.
Indeed, based on this consideration, \cite{ferrer-i-cancho-placement-2017} argued that DLM is generally optimized by SVO order, but we will see that the picture is more complex.


\begin{figure}
%\begin{center}
\textbf{A}
\begin{dependency}[theme = simple]
   \begin{deptext}[column sep=1em]
          dogs \& bite \& people  \\
   \end{deptext}
   \depedge{2}{1}{subject}
   \depedge{2}{3}{object}
\end{dependency}
\begin{dependency}[theme = simple]
   \begin{deptext}[column sep=1em]
   \japanese{犬は} \& \japanese{人を} \& \japanese{噛みます}\\ 
   inu-wa \& hito-o \& kamimasu \\
          DOG \& HUMAN \& BITES  \\
   \end{deptext}
   \depedge{3}{1}{subject}
   \depedge{3}{2}{object}
\end{dependency}

B
\begin{dependency}[theme = simple]
   \begin{deptext}[column sep=1em]
       thinks that \& big dogs \& bite \& people  \\
   \end{deptext}
    \depedge{1}{3}{subject}
   \depedge{3}{2}{subject}
   \depedge{3}{4}{object}
\end{dependency}
\begin{dependency}[theme = simple]
   \begin{deptext}[column sep=1em]
   \japanese{大きな犬が} \& \japanese{人を} \& \japanese{噛むと} \& \japanese{思います}\\ 
   ookina inu-ga \& hito-o \& kamuto \& omoimasu \\
         BIG DOG \& HUMAN \& BITES \& THINK \\
   \end{deptext}
   \depedge{3}{1}{subject}
   \depedge{3}{2}{object}
   \depedge{4}{3}{object}
\end{dependency}

C
\begin{dependency}[theme = simple]
   \begin{deptext}[column sep=1em]
   \japanese{人を} \& \japanese{噛みます}\\ 
   hito-o \& kamimasu \\
   HUMAN \& BITE  \\
   \end{deptext}
   \depedge{2}{1}{object}
\end{dependency}
\begin{dependency}[theme = simple]
   \begin{deptext}[column sep=1em]
   \japanese{犬は} \& \japanese{噛みます}\\ 
   inu-wa \& kamimasu \\
          DOG \& BITE  \\
   \end{deptext}
   \depedge{2}{1}{subject}
\end{dependency}

        \caption{A: Word order variation across languages. English (left) follows Subject-Verb-Object (SVO) order. Japanese (right) follows Subject-Object-Verb (SOV) order. Such simple sentences where both subjects and objects are expressed tend to favor SVO order. B: Embedded contexts tend to favor SOV order. C: The preference for SVO order is neutralized when only one of the two arguments is expressed. Sentences like these are fully grammatical in Japanese.}
        \label{fig:sent-dep}
\end{figure}

However, now consider what happens in more complex sentences, where verbs are embedded as dependents of other words.
In Figure~\ref{fig:sent-dep-embedded}, we consider sentences of the form `I think that big dogs bite people'.
Here, dependency length is 5 in SVO order, and 4 in SOV order, reversing the advantage  of SVO order seen in basic sentences.

One might hypothesize that simple unembedded sentences are more frequent than more complex sentences, and that the advantage of SVO in more simple sentences outweighs its disadvantage in embedded contexts.
However, this depends on the precise details of the frequency with which different syntactic configurations appear.
For example, the advantage of SVO in simple sentences is neutralized in sentences where only a subject or only an object is expressed (Figure~\ref{fig:sent-dep-1arg}).
Such sentences, while not always possible in English, are very frequent in languages like Japanese.
This means that the frequency with which different configurations appear in language use influences the degree to which SVO or SOV order are more efficient for DLM.



The reasoning above focused on SVO and SOV, but this approach can be extended to other orderings.
The crucial difference between the two orderings was whether S and O are ordered on the same or on different sides of the verb.
Other orderings can also be classified along this dimension, for instance, VSO as the third-most ordering patterns with SOV with respect to Figures \ref{fig:sent-dep}--\ref{fig:sent-dep-1arg}.

In this work, we provide evidence that languages show coadaptation between their basic word order and the frequencies with which different syntactic configurations are used.
Specifically, we contrast orderings that order S and O on the same or different sides of the verb.

Note that DLM does not make predictions as to the direction in which S and O appear: As the length of dependencies does not depend on their direction, there is no difference between, say, SOV and VOS.
Independent principles are needed to explain why SOV and SVO are more common than, say, VOS and OVS.
A key component is a strong preference for subjects to come earlier (CITE).
Subjects often are animate and given in prior discourse, and phrases with these properties generally tend to go earlier in sentences across languages (CITE).
This principle favors, say, SVO over OVS, while being equally well satisfied by SVO and SOV order.
Also cite \cite{kemmerer2012cross-linguistic}

%Now explain examples from presentations, in particular how frequency of co-expression of S and O can differentially favor SVO or SOV. {\color{blue} this is good -- i wonder if you want to illustrate the throught experiment or the basic idea of coadaptation in an opening diagram.}\\


In this paper, we test the coadaptation hypothesis using data from over 70 languages by computing, for each language, which basic word order is most efficient given its usage distribution as recorded in large-scale text data.
We then use a model of drift on phylogenetic trees to assess whether languages evolve towards states where usage distributions and actual basic word order are aligend.


\comment{at the end of the intro, it might benefit to provide a preview or plan for the rest of the paper, i.e. you could say how we plan to test the coadaptation hypothesis in x, y, z, ways, so that the later studies can be pre-empted here so as not to be very surprising.}

{\color{blue}4. related to the above paragraph -- state limitations of existing theory/approach and our proposal. Articulate how existing theories of efficient communication may be limited or insufficient to explain basic word order variation across languages, and change over time. Therefore introduce the new proposal of coadaptation. (also, if we stick with ``coadaptation'', then don't use ``co-adaptation'') }

%\mhahn{TODO fix this}

%existing proposals insufficient:



%Existing efficiency-based theories of word order variation:






%http://tedlab.mit.edu/tedlab_website/researchpapers/Gibson_et_al_2013_PsychSci.pdf

%- our results argue against work that has suggested SVO as the more efficient order (Gibson et al 2013, \url{https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4534792/})

%\url{https://www.eva.mpg.de/fileadmin/content_files/linguistics/conferences/2015-diversity-linguistics/Hammarstroem_slides.pdf}

%- Trudgill,2011

%- Gell-MannandRuhlen2011,MauritsandGriths2014

\section{Study 1: Evidence for Coadaptation}

We begin by testing whether, across languages, basic word order reflects coadaptation.
%We test the  hypotheses: Basic word order reflects coadaptation between grammar and usage (\textsc{Coadaptation}), basic word orders differ in their efficiency for DLM as proposed by \citep{ferrer-i-cancho-placement-2017} (\textsc{DifferentEfficiencies}), or there is no crosslinguistic relation between .
If there is coadaptation between basic word order and usage, we expect that languages tend to have the basic word order that is most efficient given their usage distributions.
Otherwise, we expect that, either, one of the two basic word orders is systematically more efficient for DLM, or there is no systematic relationship between basic word order and dependency length.


%First, we first use the label from WALS, which we partitioned into different-side, same-side, and free.
%Second, as a rough measure (CITE), corpus-based, whether O and S have the most-frequent direction in the treebank.


We compare two groups of word orders: SVO-like order where S and O are ordered on different sides of the verb, and SOV/VSO-like order where S and O are ordered on the same side of the verb.
Languages can fall on a spectrum between languages with entirely strict SVO order and languages with entirely strict SOV order.
English is close to one end of the spectrum with dominant SVO order, with rare exceptions (e.g., stylistically marked VS order in ``then came a dog'').
Japanese falls entirely on one end of the spectrum, allowing only SOV and OSV order.
Many languages occupy intermediate positions; for instance, in Russian, all logically possible orderings of S, V, O can occur, though with different frequencies.

We quantify the position of an individual language on this continuum using a quantitative corpus-based metric which we call {Subject-Object Symmetry}.
This is the chance that two randomly selected instances of S and O from the corpus -- not necessarily from the same sentence -- are on the same side of their respective verbs. This number is 1 in strict SOV languages like Japanese, close to 0.5 in languages with flexible word order, and close to 0 in English.

%We propose two quantitative metrics measuring where a language falls on this spectrum.
%First, we count how often S and O appear on the same side of a verb that realizes both S and O arguments ({Same-Side Count}).\mhahn{Are there better names for these measures?}
%This measure does not take into account what happens in sentences where only S or O is realized.


We compare these measures between real observed word orders and hypothetical orderings optimized for Dependency Length Minimization.
To model such hypothetical orderings, we adopt the model of {counterfactural order grammar} introduced by (CITE).
These are simple, parametric models parameterizing how the words in a syntactic structure are linearized depending on their syntactic relations.
For instance, such a grammar may specify that subjects follow or precede verbs, and that adjectival modifiers follow or precede nouns.
We use the method of (CITE) to construct grammars that approximately minimize average dependency length using stochastic gradient descent.

In order to approximately determine optimal orderings for a given language, we approximate the distribution over different syntactic configurations using large corpora.
We draw on the Universal Dependencies data, which has text data annotated with syntactic tree topologies from over 70 languages.
For each language, we construct orderings that are approximately optimized for DLM.
In order to control for variation across different optima, for each of 72 languages, we construct 12 approximately optimized grammars, and compute the average Subject-Object Symmetry across these counterfactual orderings.
\comment{** to what extent we can justify that our approach is not somewhat circular? in particular, the optimized grammars were derived from the corpora, which were used to determine the empirical frequency proportions of the basic word order -- i think some statements are needed here to suggest that there's some independence between the optimized grammar predictions by the model, and the empirical distributions.}



\comment{** Do you plan to say a bit more details about 1) the conterfactual order grammar, and 2) the data, 72 languages, where they came from, and why they are a reasonable sample, either in Methods, and/or in Appendix? I think these details would be needed for reproducibility but also for careful description of the materials and methods. also, you should say how many language families are covered in the 72. languages.}

The counterfactual order grammars have a weight in $[-1, 1]$ for every syntactic relation annotated in the corpora (e.g., subject, object).
Dependents of a head are ordered around it in order of these weights; dependents with negative weights are placed to the left of the head, others to its right.
Unlike most real languages, these grammars do not model word order freedom; accounting for it 
As in (CITE), we create multiple optimized grammars and consider the average subject-object symmetry over each grammar (a given deterministic grammar has subject-object symmetry 0 or 1).


If there is coadaptation between usage and basic word order, we expect that the subject-object symmetry of optimized orderings predicts real orderings.
Conversely, if one order is more efficient, or there is no systematic association between basic word order and DLM, then no such association is expected.


Note that, while the optimized grammars and the real orders are computed on the basis of the same corpus data, the optimized grammars are derived entirely from the tree topologies, and the original word orders do not enter their calculation (See SI Section X for results showing that results do not change when estimating optimizes grammars and real orders using disjoint subsets of corpora).




Results are shown in Figure~\ref{fig:study1}.
Without controlling for dependencies between related languages, predicted and real subject-object symmetry are correlated ($R=0.32$).
In order to account for the statistical dependencies between related languages, we then conducted a regression where we entered language families as random effects with random slopes and intercepts.
Real subject-object symmetry was strongly predictive of subject-object symmetry in the optimized grammars ($\beta = 0.44$, $SE=0.11$, $95\%$ credible interval $[0.24, 0.66]$, $P(\beta<0) < 10^{-4}$).
This result agrees with the predictions of the Coadaptation hypothesis: Languages tend to have the basic word order that is most efficient given their tree structure distributions.
It is incompatible with the suggestion that SVO is generally more efficient for DLM \citep{ferrer-i-cancho-placement-2017}.
%It is incompatible with the \textsc{MultipleOptima} or \textsc{NoOptimization} hypotheses.

%\comment{should say a few more details, e.g. why you chose a mixed effect model, for instance, you could say in order to control for x, y, z, we performed a mixed-effect ...}

%\comment{Somewhere in introduction, we should probably single out a paragraph and make it explicit the idea of the "coadaptation hypothesis: xyz....", so that we can keep referring to that phrase later in results.}

%{\color{blue} If you plan to go for NHB, then you can encapsulate some methodologies in the Methods section and focus on explaining and interpreting the main results.}


\begin{figure}
    \centering
    \includegraphics[width=0.95\textwidth]{figures/fracion-optimized_DLM_2.6.pdf}
    \caption{Study 1: Subject-object symmetry and real and optimized orderings. x-axis real order distributions, y-axis model prediction. \comment{can you increase the fontsize of the axes labels? also i wonder if it's possible to increase the font size of the languages too, and perhaps get rid of the grids? each langauge has 2.6 next to it -- seems redundant; remove?}}
    \label{fig:study1}
\end{figure}


%\section{Analysis 2: Spoken Corpora}

%\comment{maybe somewhere here or nearby you should acknolwedge that there are only very limited number spoken corpora available, but nevertheless we found these and suggest that these languages span the spectrum of word order variation.}


We have approximated the usage distribution over syntactic configurations using corpora.
However, a large part of annotated corpus data is written (e.g., news and web text), whereas spoken language is thought to be most important for language change, particularly as widespread literacy is very recent.
There are only a small number of spoken corpora with syntactic annotation.
We collected all spoken corpora in dependency format that we could obtain access to (see Materials and Methods), and tested whether the conclusions of Analysis 1 continued to hold when using existing dependency treebanks that consist of transcribed spoken text.
Results on these treebanks are shown in Figure~\ref{fig:spoken}.
In a linear regression with real subject-object symmetry as the dependent variable, model output predicts real values ($\beta=1.11$, $SE=0.29$, $95\%$ CrI $[0.58, 1.69]$).\footnote{The small number of datapoints does not support a mixed-effects analysis. Even with strong priors and more iterations, we encountered small values of Tail Effective Sample Size. However, we found qualitatively similar results when excluding either English or Norwegian, in which case all languages come from different families, and the simple linear model is fully appropriate.}.
This result is in agreement with that from the full set of corpora.
%\mhahn{TODO the other measure}
%\mhahn{what is the dependent and independent variables in the linear regression? (and perhaps why a Bayes LR as opposed to a plain LR? need to justify a bit more}

\begin{figure}
    \centering
    \includegraphics[width=0.95\textwidth]{analysis_spoken/spoken.pdf}
    \caption{Comparing subject-object symmetry using spoken corpora. We show the corresponding points using the full UD data in gray.}
    \label{fig:spoken}
\end{figure}



\section{Analysis 3: Historical Evolution}


If variation in word order reflects coadapation between grammar and usage, we should expect to see coevolution of basic word order and usage as languages change.

To test this, we conducted a phylogenetic analysis of the evolution of usage and word order.

%We also drew on languages where dependency treebanks with data from different stages of the same language are available (see Materials and Methods for details).



The diagonal in Figure~\ref{fig:study1} describes those points where usage and grammar match perfectly in basic word order, i.e., those points where the real order distribution is identical to the order distribution among optimized grammars, given the distribution over tree topologies.
Therefore, if the coadaptation hypothesis is true, then languages should stay close to or move towards the diagonal in the plane described by predicted and real subject-object symmetry.



We used a model of drift on phylogenetic trees to quantify the evidence for coadaptation.
We modeled how usage (optimal subject-object symmetry) and grammar (real subject-objectn symmetry) evolve over time.
We adopted an Ornstein-Uhlenbeck process (CITE).
This model considers how the two quantities change responding both to random fluctuations and to systematic drift towards certain regions of the plane.

Under the coadaptation hypothesis, drift in usage and grammar should be correlated, so that languages move towards the diagonal.


We used phylogenetic trees from (CITE) and .
We modified these trees by adding historical stages of existing languages for which we have corpora, such as Old English (approx. 900 AD) as an ancestor of English.


In the resulting model, drift in the two directions was positively correlated with high confidence (posterior probability of negative or no correlation XXX).
We compared the model with a simpler version in which usage and grammar evolve independently, finding that the full model provided better model fit (Bayes Factor XXX).

\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{change/visualize/stationary.pdf}
    \caption{Drift directions and long-term stationary distribution according to the phylogenetic drift model.
    The blue distribution indicates the estimated long-term stationary distribution of languages, indicating where languages tend to lie after millenia of evolution.
    The red dots indicate sampled directions of movement for individual languages at the black dots over TODO years.}
    \label{fig:drift-model}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{TODO}
    \caption{The phylogenetic tree used in our analysis.}
    \label{fig:phylogenetic-tree}
\end{figure}

We visualize model fit in Figure~\ref{fig:drift-model}.
The model predicts that languages will, over their evolution concentrate in such a way that grammar and usage come to be positively correlated.

We also indicate expected direction of future changes in three languages (English, Russian, Japanese). TODO

We visualize the evolution of languages where multiple historical stages are documented in Figure~\ref{fig:historical}.
The sample includes multiple languages that started with flexible order and moved towards SVO (English, French, Spanish, Russian, Bulgarian, Greek), languages that have remained SVO (Mandarin and Cantonese), and languages that have remained predominantly SOV (Hindi/Urdu).

In the case of the languages that have remained SVO (Cantonese and Mandarin), predicted subject-object symmetry moved towards its true values; that is, these languages appear to have evolved their usage patterns in a way that makes their basic word order more efficient.
Hindi/Urdu, which have remained predominantly SOV, show little movement in either dimension, slightly away from the diagonal.
Finally, among the remaining languages, which started with flexible order and moved towards SVO, all moved along or towards the diagonal.

%\mhahn{is there a way to also quantify this result?}
%\comment{** yes it might be desirable and even necessary to quantify these results -- is there any alignment metric we can use to determine how well the historical shift aligns with the diagonal, or S-O symmetry reference points? and then compare that alignment metric to some chance value, e.g. by shuffling data across time or era---use this as a null distribution; then report the p-value of the observed, non-shuffled cases? you can then juxtapose the null trajectories (mean + confidence band, in very light color) on the plot to show people how the real trajectory of historical change deviates from the null.}

%We compare to counterfactual trajectories where
%- languages changed only in usage or in order
%- we 


Beyond these observations, the model (and the coadaptation hypothesis) predicts that a language that changed from SVO towards SOV should have correspondingly moved towards the top-right of the plane. 
Currently, no annotated treebanks are available for such a language to directly test this prediction; our prediction can be tested once such a treebank becomes available.



\begin{figure}
    \centering
    \includegraphics[width=0.95\textwidth]{figures/historical_2.6_times.pdf}
    \caption{Study 1 (historical changes) \comment{similar comment -- worth improving the aesthetics of the figure, e.g. get rid of grids and gray background ... etc. also need to explain what are the gray dots in the background.}}
    \label{fig:historical}
\end{figure}




\section{Analysis 4: Co-Expression of Subjects and Objects}

\comment{description here seems a bit brief; elaborate more? or say in what way is this result interesting, or surprising, or novel (to the literature)?}

So far, our results provide evidence for coadaptation between usage and basic word order in the evolution of language.
In what aspects of usage patterns do languages differ and change to realize this coadaptation?
That is, in what ways do usage patterns influence which basic word order is optimally efficient?
Based on the discussion in the introduction, we hypothesized that languages favor SOV-like orders more when they do not frequently coexpress subjects and objects on a single verb.

We quantified the frequency of co-expression of subjects and objects by calculating what fraction of all verbs that realize at least a subject or an object simultaneously realize both.

We show this ratio together with subject-object symmetry in Figure~\ref{fig:study2}.
In a linear mixed-effects models, with by-family intercept and slope, subject-object symmetry was predictive of this fraction ($\beta=-0.11$, $SE=0.05$, $95\%$ CrI $[-0.21, -0.01]$).
%\mhahn{TODO the other measure}

\mhahn{also run this through the phylogenetic model in addition to this plain regression}

{\color{blue}Did you mention that there were some novel discoveries or generalizations, do you want to mention them here?}

\begin{figure}
    \centering
    \includegraphics[width=0.95\textwidth]{figures/objects-order-pureud-byVerb.pdf}
    \caption{Study 2 (Coexpression of suibjects and objects) \comment{ditto -- consider getting rid of the grids and gray background? also maybe improve the aesthetics of this figure, and add a legend (e.g. black box for spoken, gray box for written equivalent?)}}
    \label{fig:study2}
\end{figure}


%\section{Hypotheses}

%SVO preferred when S,O both present, and when O long and S short.

%SOV, VSO orders preferred when O short/not present/intransitive, and when embedded

%Relevant data

%- intransitive subjects behaving like objects

%- Continental West Germanic (except Yiddish). Main clauses predominant SVO/OSV; embedded clauses SOV/OSV.

%- languages with predominant VSO, but alternative SVO in matrix clauses: Standard Arabic, Berber, Ancient Egyptian

%- relative clauses in Bantu: Demuth,Katherine,andCarolynHarford.1999.Verb raising and subject inversion in Bantu relatives. Journal of African Languages and Lingustics20:41ñ61.


\section{Discussion}

{\color{blue} 1. how this work impacts the research on basic word order, and particularly, extends the theory of efficient communication and suggests new functional pressures in shaping the evolution of syntactic structure.}\\

{\color{blue} 2. how this work is limited.}

\paragraph{Causal direction} can't decide. not logically necessary that there even is a single causal direction across languages and time.


(There is a linguistic debate on the classification of languages into discrete basic word order types. We sidestep this issue by acknowledging that there is a continuous spectrum).


\paragraph{Implications for efficiency-based explanations of language structure}
Variation between the grammars of different languages, and change of languages over time, pose an interesting question for efficiency-based explanations.
When languages differ, this could be because they reflect different optima or points along a Pareto frontier \citep{zaslavsky2018efficient, hahn2020universals}.
As languages change, they move along a Pareto frontier of optimal points \citep{zaslavsky2019evolution}.
However, it could also be because the way language is used differs between two languages, making different grammars most efficient given the ways languages are used differently.
As a language changes, change in grammatical structure then has to be accompanied by change in the way the language is used, i.e., there is a process of {coadaptation} between grammar and use.
In a different domain, \cite{gibson2017color} suggest that color naming systems are efficient given the usefulness of color in a society.

\paragraph{More detailed discussion of previous work (should partly go into intro?}
There are a range of previous theories of basic word order variation.
\cite{maurits2010why} propose that the frequency of different basic word order pattern is predicted by Uniform Information Density, the idea that information in language is distributed in such a way as to avoid peaks and troughs in the rate at which information is transmitted.
Their model predicts object-initial order to be strongly dispreferred.
However, it also suggests SVO to be more considerably more efficient than SOV, even on usage data from an SOV language (Japanese), in contrast with the empirically observed distribution.

%(Gell-Mann  &  Ruhlen,  2011;  Givón,  1979;  Newmeyer,  2000a,  2000b).
% (Senghas,  Coppola,  Newport,  &  Supalla,  1997
% Sandler, Meir, Padden, & Aronoff, 2005
% Goldin-Meadow, So, Ozyurek, and Mylander (2008) 
% (Goldin-Meadow  et  al.,  2008), and Italian (Langus & Nespor, 2010)
%Gershkoff-Stowe L, Goldin-Medow S (2002) Is there a natural order for expressingsemantic relations?Cognit Psychol45(3):375–412.13. 
%Sandler W, Meir I, Padden C, Aronoff M (2005) The emergence of grammar: Sys-tematic structure in a new language.Proc Natl Acad Sci USA102(7):2661–2665.14. 
%Goldin-Meadow S, So WC, Ozyürek A, Mylander C (2008) The natural order of events:How speakers of different languages represent events nonverbally.Proc Natl Acad SciUSA105(27):9163–9168.15. %Langus A, Nespor M (2010) Cognitive systems struggling for word order.CognitPsychol60(4):291–318
\cite{gibson-noisy-channel-2013} argue that SVO order is more robust under noise in communication for semantically reversible events, because deletion of one argument due to noise makes meaning recovery easier when arguments are on different sides of the verb.
Under this account, the high prevalence of SOV order is explained by the idea that SOV is the more default order, and that SVO order later results from shift.
Indeed, there is evidence that SOV emerges spontaneously in gestural communication (CITE).
\cite{maurits2014tracing} apply phylogenetic modeling to infer how frequently different changes in basic word order are.
They do not find evidence that one of SOV or SVO is favored over the other in language change, and that languages can cycle between these two orders over time.
They do, however, find evidence that ancestral languages of multiple families were SOV.

\cite{ferrer-i-cancho-placement-2017} argues that the variation is caused by a tension between optimizing DLM (thought to favor SVO) and making the verb predictable (thought to favor SOV). This hypothesis is predicated on the idea that DLM favors SVO, which our empirical results will show is not true in general.


\section{Conclusion}

\section*{Methods}

{\color{blue}Detailed methods for reproducing this work are typically written in the last section in a NHB article. Include data and code repo and explicit statement that will support replicating all the findings.}\\

{\color{blue}Prepare supplementary material if need be, e.g., you might want to insert a table and map of all languages and their families, dates or periods of time covered, and their word order(s) etc, as well as the detailed experimental parameters.}



\paragraph{Ordering Grammars}


%- flexible

%$a = \sum_i a_{x_i}$

%where $x$ is a feature vector encoding relevant properties of the word. Concretely, we choose the following:
%- dependency label and POS tag
%- dependency label and POS tag and length of the constituent
%- for each sibling, its dependency label + POS tag + length of the constituent

While (CITE) introduced this stochastic parameterization to enable gradient-based optimization, we use it to model word order flexibility.

\paragraph{Creating Optimized Grammars}


\paragraph{Data}


In addition the data available in Universal Dependencies, we added an Old English dependency treebank in a slightly different but comparable version of dependency grammar (CITE). (While there are some other historical treebanks such as Middle English, they are not in the dependency format, and calculating dependency length is highly nontrivial without a high-quality conversion).
As the Ancient Greek data spans approximately millenium, we split it in three conventional stages (Archaic 700--500 BC, Classical 500--300BC, and Koine 300BC--300AD, CITE). \citep{taylor1994change}
We excluded medieval Latin data as it does not reflect a living language.




(Hindi/Urdu are a single language linguistically, but they have separate treebanks in the available datasets.)

Spoken Data:
For four languages, there are spoken treebanks in the UD project (Slovenian, Naija, Norwegian, French).
For Japanese, we used Tueba J/S, a treebank of telephone conversations.
For English, we used an automated conversion (CITE) of the Switchboard section of the Penn Treebank (CITE) to Universal Dependencies.

\paragraph{Model of Change}

% Bayesian inference for Markov processes with diffusion and discrete components
% https://onlinelibrary-wiley-com.stanford.idm.oclc.org/doi/full/10.1111/biom.13292 Latent Ornstein‐Uhlenbeck models for Bayesian analysis of multivariate longitudinal categorical responses
%- Brownian motion (Phylogenetic Independent Contrasts): Model the direction of change. 

%\begin{equation}
%    dX_t = A dB_t
%\end{equation}

We adopt the model of Brownian motion on phylogenetic trees.
As the data for optimized grammars comes in the form of counts, we model the usage of a language $L$ as a latent variable $X_L$ that defines the log-odds that an optimized grammar has a subject-object symmetry of 1 (as opposed to 0).
We model the state of the grammar as $Y_L$, the observed subject-object symmetry.
$X_L$ and $Y_L$ together form the state $\xi_L \in \mathbb{R} \times [0,1]$  of the language.
We model the instantaneous change of the state $\xi_t$ a language at a given time $t$ as Brownian motion:
\begin{equation*}
    d\xi_t = \Lambda dB_t
\end{equation*}
where $\Lambda \in \mathbb{R}^{2\times 2}$.
$\Lambda_{1,2}$ encodes the extent to which drift of $X_t$ and $Y_t$ is correlated:
$A_{1,2} > 0$ means that languages drift in such a way that $X_L, Y_L$ are positively correlated (coadaptation); $A_{0,0} = 0$ means that movement in both directions is not correlated.


\bibliography{literature}
\bibliographystyle{natbib}

\appendix

\section{Historical Languages}



\begin{tabular}{llll} \hline
Chinese (Cantonese) & Classical Chinese & -400  \\
& Cantonese & +2000\\ \hline
Chinese (Mandarin) & Classical Chinese (-400)  \\
& Mandarin & +2000 \\ \hline
East Slavic & Old Russian & +1200 \\
& Russian & +2000 \\ \hline
English & Old English & +900 \\
& English  & +2000\\ \hline
French & Latin &+0  \\
& Old French &+1200\\
& French  & +2000\\ \hline
Greek & Ancient Greek & +0 \\
& Greek  & +2000\\ \hline
Hindi/Urdu & Sanskrit & -200 \\
& Hindi  & +2000\\
& Urdu  & +2000\\ \hline
South Slavic & Old Church Slavonic & +800 \\
& Bulgarian  & +2000\\ \hline
Spanish & Latin &+0 \\
& Spanish  & +2000\\ \hline
\end{tabular}

\section{Phylogenetic Tree}

We obtained tree topologies from Glottolog (CITE).
We only retained interior nodes when more than one of their daughter nodes had languages occurring in our dataset.

TODO print the tree

We labeled interior nodes for time using estimates based on historical evidence and the linguistic literature.



\begin{tabular}{lllllll}
Group & Split & Source or Rationale \\ \hline
Afroasiatic & 10,000 BC & CITE \\
Indo-European & 4,000 BC & CITE \\
Semitic & 3,500 BC & CITE \\
Insular Celtic & 500BC & Sound shift k$^w$ $>$ p in Brythonic antedates attestation of name `Britain'\\ % Gray and Atkinson: 900BC. Language-tree divergence times support.. Rexova et al, Cladistic analysis
Common Turkic & 500AD & Antedates Orkhon Turkic (7th c.), true date might be earlier. \\
Balto-Slavic & 1,500 BC & TODO refs in wiki Balto-Slavic \\
Slavic       & 500AD & TODO refs in wiki Balto-Slavic\\
Indo-Iranian & 2,000 BC \\ % Parpola 1999 suggests 2000 BC. The formation of the Aryan branch of Indo-European.
Atlantic-Congo & 3,000 BC \\
Uralic & 4,000 BC & Maurits \\
West-Semitic & 2,000 BC & \\
Central-Semitic & 1,500 BC & \\
South-Slavic & 600 BC & Expansion of Slavic into Balkan, but antedates Old Church Slavonic \\
West-Slavic & 600 BC & Expansion of Slavic \\
Iranian & 1000 BC \\ % Kurmanji, Persian. Parpola 1999 suggests 1900 BC.
Nothern-Atlantic & 0 AD \\
Baltic & 0 AD & TODO refs in wiki Balto-Slavic\\
Ugric & 2,000 BC & Maurits \\
Hindustani & 1,800 AD & Standardization of Hindi and Urdu\\
Serbo-Croatian & 1,900 AD & Standardization of Serbain and Croatian\\
West-Germanic & 500 AD & Migrations into Britain and southern central Europe\\
North-Germanic & 600 AD & Split of Old Norse into regional variants. \\% (e.g. assimilation of nasals to following stop in Western Norse in the 7th century. Bandle 2005, Ch. XVII par. 202, The typological ...I Phonology Old East Nordic, p. 1856, 1859. \\
West-Scandinavian & 1,100 AD & Sound shifts specific to Norwegian\\
Icelandic-Faroese & 1,200 AD & Sound shifts specific to Faroese (Petersen, Hjalmar, The change of TH to h in Faroese) \\
Goidelic & 1,300 AD & Migrations from Ireland to Scotland \\
Brythonic & 400 AD & Migrations from Britain to Bretagne \\
Global Dutch & 1,600 AD & Dutch colony in South Africa \\
Western South Slavic & 1,000 AD & Antedates earliest Slovenian and Serbo-Croatian texts\\
Western Romance & 800 AD & Expansion of Christian kingdoms into Iberia \\
Italo-Western-Romance & 500 AD & End of the Western Roman empire \\
Iberian Romance & 1,000 AD & Expansion of Christian kingdoms in Iberia, earliest Iberian Romance texts \\
West Iberian & 1,100 & Independence of Portugal \\
\end{tabular}

Dates of split are not always strictly meaningful (e.g. for Romance).

\section{Details for Phylogenetic Models}



We adopt the model of random walks on phylogenetic trees.
We model the development of grammar and usage as an Ornstein-Uhlenbeck process (See SI Section X for results with a simpler Brownian motion model).
According to the model of random walks on phylogenetic trees, whenever a (historical) language branches off into multiple successor languages, they independently develop according to this process.


As the data for optimized grammars comes in the form of counts, we model the usage of a language $L$ as a latent variable $X_L$ that defines the log-odds that an optimized grammar has a subject-object symmetry of 1 (as opposed to 0).
We model the state of the grammar as $Y_L$, the observed subject-object symmetry.
$X_L$ and $Y_L$ together form the state $\xi_L \in \mathbb{R} \times [0,1]$  of the language.

\subsection{Simple Brownian Motion Model}


We model the instantaneous change of the state $\xi_t$ a language at a given time $t$ as a correlated Brownian motion in two dimensions:
\begin{equation*}
    d\xi_t = \Lambda dB_t
\end{equation*}
where $\Lambda \in \mathbb{R}^{2\times 2}$ is non-degenerate.

\subsection{Model with Drift}

In our setting, the Brownian motion model has the unrealistic property that trajectories $\xi_t$ tend to move arbitrarily far away from the origin over time $t$, whereas $\xi_t$ in reality remains bounded.
(See below for likelihood results, showing poor fit of the simple Brownian model).

This limitation can be addressed by adding a drift term, whereby languages tend to stay within a bounded area, while subjected to random walk dynamics within this area.

This is expressed by the following stochastic differential equation, known as the Ornstein-Uhlenbeck process:
%We model the instantaneous change of the state $\xi_t$ a language at a given time $t$ as a combination of drift and Brownian motion:
\begin{equation*}
    d\xi_t = -A(\xi_t-\mu) + \Lambda dB_t
\end{equation*}
where $A, \Lambda \in \mathbb{R}^{2\times 2}$, all eigenvalues of $A$ have positive real part, and $\Lambda$ has full rank.


The matrix $A$ encodes the direction of drift.
$A_{1,2}$ encodes the extent to which drift of $X_t$ and $Y_t$ is correlated:
$A_{1,2} > 0$ means that languages drift in such a way that $X_L, Y_L$ are positively correlated (coadaptation); $A_{0,0} = 0$ means that languages do not drift towards a correlation (See Si Section X for an alternative formulation in terms of correlated Brownian motion).


We model the instantaneous change of the state $\xi_t \in \mathbb{R}^2$ of a language at a given time $t$ as a combination of drift and Brownian motion, formalized by the following stochastic differential equation defining the Ornstein-Uhlenbeck process \citep[p. 109, eq. 4.4.42]{gardiner1983handbook}:
\begin{equation*}
    d\xi_t = -\Gamma \cdot (\xi_t-\mu) dt + \Lambda \cdot dB_t
\end{equation*}
where $\mu \in \mathbb{R}^2$, $\Gamma, \Lambda \in \mathbb{R}^{2\times 2}$, the real part of each eigenvalue of $\Gamma$ is positive, and $\Lambda$ has full rank (so that $\Lambda\Lambda^T$ is positive definite).
%is positive definite, and $\Lambda$ is diagonal with positive entries (see SI Section for results with non-diagonal $\Lambda$).


The matrix $A$ encodes the direction of drift.
$A_{1,2}$ encodes the extent to which drift of $X_t$ and $Y_t$ is correlated:
$A_{1,2} > 0$ means that languages drift in such a way that $X_L, Y_L$ are positively correlated (coadaptation); $A_{0,0} = 0$ means that languages do not drift towards a correlation (See Si Section X for an alternative formulation in terms of correlated Brownian motion).

\subsection{Implementation}

We implemented the models in Stan and obtained posterior samples using MCMC. TODO details.

We computed marginal likelihoods using Stepping Stone Sampling with $K=10$ stones.
We verified stability of the estimates by running the procedure ten times for each model.

\paragraph{Computing Likelihood for Brownian Motion Model}

We impose an LKJ prior on the correlation matrix of $A$, a student $t$ prior on the variance components of $A$ and $\Lambda$, a ... prior on the two components of $\Lambda$.

\paragraph{Computing Likelihood for Ornstein-Uhlenbeck Model}

Then the conditional distribtion is given by \citep[Theorem 3.3]{schach1971weak}, \citep{gardiner1983handbook}, \citep[p. 156, eq. 6.124]{risken1989fokker}

\begin{equation}
\xi_{t+\Delta} | \xi_t \sim N\left(\mu + \exp(-\Delta \Gamma) (\xi_t-\mu), \Omega - \exp(-\Delta \Gamma) \Omega \exp(\Delta \Gamma^T) \right)
\end{equation}
where the matrix $\Omega$ is obtained as the solution of the equation \citep[p. 110, eq. 4.4.51]{gardiner1983handbook} \citep[p. 156, eq. 6.126]{risken1989fokker}:
\begin{equation}
    \Gamma\Omega+\Omega\Gamma^T = \Lambda \Lambda^T
\end{equation}
The stationary distribution is
\begin{equation}
\xi_{t} \sim N\left(\mu, \Omega \right)
\end{equation}


\paragraph{Accounting for Areal Effects}
We accounted for effects of geographic neighborhoods by modeling linguistic areas.
We modeled linguistic areas as latent variables defining location-dependent values $\mu(x)$ (where $x$ is a point on the surface of the earth) that languages drift towards.

We modeled the grammar and usage components of $\mu$ to depend on the language's geographic position.
We placed a Gaussian process prior with a Laplace kernel on $\mu$. 
That is, the covariance between $\mu$ at points $x, y$ on the surface of the earth was taken to be
\begin{equation}
    Cov(\mu_x, \mu_y) = \alpha \exp\left(-\frac{1}{\rho^2} d(x,y)\right)
\end{equation}
where $d(x,y)$ is the great circle (geodesic) distance between points $x, y$.
This kernel is positive-definite (whereas the commonly used RBF kernel is not positive-definite with the geodesic distance $d(\cdot, \cdot)$).

We extracted locations of languages from WALS.
For ancestors, we defined their location as the mean of the locations of their immediate children.


We perform inference with MCMC TODO.

In Table, we show all models together with the logarithm of the estimated marginal likelihood of the data.
Higher marginal likelihood indicates better model fit.

\begin{table}
\begin{tabular}{lllll}
Model & SDE & File & Log-Likelihood & loo \\ \hline
Uncorrelated   & $d\xi_t = \left(\begin{matrix} \sigma_1 & 0 \\ 0 & \sigma_2\end{matrix}\right) dW_t$ & 30.stan & -239\\
Correlated  & $d\xi_t = \left(\begin{matrix} \sigma_1 & \rho_1 \\ \rho_2 & \sigma_2\end{matrix}\right) dW_t$ & 27, 28, 29 & -197, -220, -195 \\
\hline
Correlated Drift & $d\xi_t = - \left(\begin{matrix} \gamma_1 & \gamma_{1,2} \\ \gamma_{2,1} & \gamma_{2,2}\end{matrix}\right) \left(\xi_t-\mu\right) dt + \left(\begin{matrix} \sigma_1 & 0 \\ 0 & \sigma_2\end{matrix}\right) dW_t$ & 28.stan & -297\\
Correlated + Drift & $d\xi_t = - \left(\begin{matrix} \gamma_1 & 0 \\ 0 & \gamma_{2,2}\end{matrix}\right) \left(\xi_t-\mu\right) dt + \left(\begin{matrix} \sigma_1 & \rho_1 \\ \rho_2 & \sigma_2\end{matrix}\right) dW_t$ & 26.stan & -303 \\
Uncorrelated + Drift & $d\xi_t = - \left(\begin{matrix} \gamma_1 & 0 \\ 0 & \gamma_{2,2}\end{matrix}\right) \left(\xi_t-\mu\right) dt + \left(\begin{matrix} \sigma_1 & 0 \\ 0 & \sigma_2\end{matrix}\right) dW_t$ & 25.stan & -343 \\
Correlated Drift & $d\xi_t = - \left(\begin{matrix} \gamma_1 & \gamma_{1,2} \\ \gamma_{2,1} & \gamma_{2,2}\end{matrix}\right) \left(\xi_t-\mu\right) dt + \left(\begin{matrix} \sigma_1 & \rho_1 \\ \rho_2 & \sigma_2\end{matrix}\right) dW_t$ & 27.stan & -297\\
\hline
Correlated  + Areas & $d\xi_t = - \left(\begin{matrix} \gamma_1 & 0 \\ 0 & \gamma_2\end{matrix}\right) \left(\xi_t-\mu(x)\right) dt + \left(\begin{matrix} \sigma_1 & \rho_1 \\ \rho_2 & \sigma_2\end{matrix}\right) dW_t$ & 29, 31, 32, 33 & -61, -69\\
Uncorrelated  + Areas & $d\xi_t = - \left(\begin{matrix} \gamma_1 & 0 \\ 0 & \gamma_2\end{matrix}\right) \left(\xi_t-\mu(x)\right) dt + \left(\begin{matrix} \sigma_1 & 0 \\ 0 & \sigma_2\end{matrix}\right) dW_t$ & 30 & 113\\
\hline
Correlated  + Area(t) & $d\xi_t = - \left(\begin{matrix} \gamma_1 & 0 \\ 0 & \gamma_2\end{matrix}\right) \left(\xi_t-\mu(x)\right) dt + \left(\begin{matrix} \sigma_1 & \rho_1 \\ \rho_2 & \sigma_2\end{matrix}\right) dW_t$ & 35, 36, 38 & -47, -72, -54\\
Uncorrelated  + Area(t) & $d\xi_t = - \left(\begin{matrix} \gamma_1 & 0 \\ 0 & \gamma_2\end{matrix}\right) \left(\xi_t-\mu(x)\right) dt + \left(\begin{matrix} \sigma_1 & 0 \\ 0 & \sigma_2\end{matrix}\right) dW_t$ & 37 & -109\\
\hline
\end{tabular}
\caption{Phylogenetic drift models. For each model, we provide a representation as a stochastic differential equation, and the logarithm of the estimated marginal likelihood.}
\end{table}

\begin{table}[]
    \centering
    \begin{tabular}{c|c}
   Parameter & Prior \\ 
  $\left(\begin{matrix} \sigma_1 & 0 \\ 0 & \sigma_2\end{matrix}\right)$       &  \\
         $\left(\begin{matrix} \sigma_1 & \rho_1 \\ \rho_2 & \sigma_2\end{matrix}\right)$ & 
    \end{tabular}
    \caption{Caption}
    \label{tab:my_label}
\end{table}


Conditions:

- $\Gamma$ has positive eigenvalues

- $\Gamma\Omega+\Omega\Gamma^T$ is a covariance matrix

- $\Omega$ is a covariance matrix

Things to visualize

- stationary distribution

- instantaneous distribution over change directions


\section{Brownian Motion Model}

Conceptually, a disadvantage of the Brownian motion model is that it does not have a long-term stationary distribution.

% https://courses.helsinki.fi/sites/default/files/course-material/4523939/Freckleton02.pdf

\end{document}



