\documentclass[11pt,a4paper]{article}
\usepackage{times,latexsym}
\usepackage{url}
\usepackage[T1]{fontenc}
\usepackage{grffile}

\usepackage{CJKutf8}
\usepackage{fullpage}

\usepackage{tikz-dependency}

\newcommand\BibTeX{B{\sc ib}\TeX}
\newcommand\confname{EMNLP-IJCNLP 2019}
\newcommand\conforg{SIGDAT}

\usepackage{longtable}

\usepackage{amsmath}
\usepackage{tikz-dependency}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator{\E}{\mathop{\mathbb{E}}}

\usepackage{amssymb}% http://ctan.org/pkg/amssymb
\usepackage{pifont}% http://ctan.org/pkg/pifont
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%


\newcommand{\Prob}{\mathbb{P}}%

%\usepackage{pslatex}
%\usepackage{latexsym}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{bm}
\usepackage{graphicx}
\usepackage{tikz}
\usepackage{xcolor}
\usepackage{url}
%\usepackage[colorinlistoftodos]{todonotes}
\usepackage{rotating}
\usepackage{multirow}



\newcommand{\japanese}[1]{\begin{CJK}{UTF8}{min}#1\end{CJK}}


\usepackage[T1]{fontenc}

\usepackage{pslatex}
%\usepackage{latexsym}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{bm}
\usepackage{graphicx}
\usepackage{tikz}
\usepackage{xcolor}
\usepackage{url}
%\usepackage[colorinlistoftodos]{todonotes}
\usepackage{rotating}
%\usepackage{natbib}
\usepackage{amssymb}



\usepackage{natbib}
\bibliographystyle{plainnat}

\usepackage{amsthm}
 

\allowdisplaybreaks

\newcounter{theorem}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{question}[theorem]{Question}
\newtheorem{example}[theorem]{Example}
\newtheorem{defin}[theorem]{Definition}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{thm}[theorem]{Theorem}


\newcommand{\R}[0]{\mathbb{R}}
\newcommand{\Ff}[0]{\mathcal{F}}
%\newcommand{}[1]{\textbf{#1}}


\newcommand{\soft}[1]{}
\newcommand{\nopreview}[1]{}
\newcommand\comment[1]{{\color{red}#1}}
\newcommand\mhahn[1]{{\color{red}(#1)}}

\newcommand{\thetad}[0]{{\theta_d}}
\newcommand{\thetal}[0]{{\theta_{LM}}}
\newcommand{\thetap}[0]{{\theta_{P}}}


\title{Supplementary Information: A Coadaptation Theory of the Evolution of Word Order}
% Maybe?: Grammar and Usage Interact in the Evolution of Language

\author{Michael Hahn\\Department of Linguistics, Stanford University\\\\Yang Xu\\ Department of Computer Science, Cognitive Science Program, University of Toronto\\\\ \texttt{mhahn2@stanford.edu}}

\date{}

\begin{document}
\maketitle

\tableofcontents

\section{Historical Languages}

These are the historical languages in our dataset, with approximate dating assigned:


\begin{tabular}{llp{10cm}llll}
Language & Time & Rationale \\ \hline
Coptic & 400 AD & Dating of the Apophthegmata Patrum texts used in the UD treebank\\
Gothic & 350 AD & Life of bible translator Ulfilas (311--383)\\
Old Russian & 1200 AD & Approximate mean age of texts used\\
Old French & 1200 AD  & Approximate mean age of texts used\\
Old English & 900 AD & Approximate mean age of texts used \\
Sanskrit & 900BC & Approximate mean age of texts used \\
Classical Chinese & 300 BC & Life of Mengzi (died around 300 BC); the treebank contains his teachings as collected by his followers. \\
Latin & 0 AD & Approximate mean age of texts used \\
Archaic Greek & 700 BC & Approximate mean age of texts used \\
Classical Greek & 400 BC & Approximate mean age of texts used \\
Koine Greek & 0 AD  & Approximate mean age of texts used\\
Old Church Slavonic & 850 AD &  Bible translation after invention of Glagolitic alphabet around 850 AD. \\
\end{tabular}



We included the following languages in Figure (TODO):

\begin{tabular}{llll} \hline
Family & Languages & Dates \\ \hline\hline
Chinese & Classical Chinese & -400  \\
& Cantonese & +2000\\ 
& Mandarin & +2000 \\ \hline
East Slavic & Old Russian & +1200 \\
& Russian & +2000 \\ \hline
English & Old English & +900 \\
& English  & +2000\\ \hline
Romance & Latin &+0  \\
& Old French &+1200\\
& French  & +2000\\
& Italian & +2000\\
& Spanish & +2000\\
& Catalan & +2000\\
& Galician & +2000\\
& Portuguese & +2000\\
& Romanian & +2000\\ \hline
Greek & Archaic Greek & -700 \\
      & Classical Greek & -400 \\
      & Koine Greek & +0\\
& Greek  & +2000\\ \hline
Indo-Aryan & Sanskrit & -200 \\
& Hindi  & +2000\\
& Urdu  & +2000\\ \hline
Eastern South Slavic & Old Church Slavonic & +800 \\
& Bulgarian  & +2000\\ \hline
\end{tabular}


\subsection{Corpora and Corpus Sizes}

As described in Methods, we included all UD 2.6 languages with at least 10,000 available words, and the ISWOC Old English treebank.

The included UD languages have the following overall sizes:

\begin{longtable}{lllll}
Language & Sentences & Words \\ \hline
\input{../fitGrammars/corpora}
\end{longtable}

The following UD languages did not meet the inclusion criterion:

\begin{longtable}{lllll}
Language & Sentences & Words \\ \hline
\input{../fitGrammars/excluded}
\end{longtable}


Furthermore, we excluded the following treebanks from languages otherwise included:

\begin{tabular}{lllll}
Treebank & Rationale \\ \hline
Chinese-CFL & Text written by non-native speakers\\
English-ESL & Text written by non-native speakers \\
French-FQB & Consists entirely of questions \\
Hindi English-HIENCS &  Consists of codeswitched text \\
Latin-ITTB & Consists of Medieval Latin text \\
Latin-LLCT & Consists of Medieval Latin text  \\
English-Pronouns & Specifically targets pronouns \\
\end{tabular}



The spoken corpora have the following overall sizes (for Naija, see above):

\begin{longtable}{lllll}
Language & Sentences & Words \\ \hline
\input{../optimizeDLM/SpokenJapanese/corpora}
\input{../optimizeDLM/UD_Spoken/corpora}
\input{../optimizeDLM/Switchboard/corpora}
\end{longtable}

\section{Phylogenetic Tree}

\subsection{Tree Topology}

We obtained tree topologies from Glottolog~\citep{nordhoff2011glottolog}.
We only retained interior nodes when more than one of their daughter nodes had languages occurring in our dataset.
The resulting tree topology is displayed in Figure~\ref{fig:tree}.\footnote{Tree obtained with https://icytree.org/.}


\begin{figure}
    \centering
	\includegraphics[width=0.9\textwidth]{../trees/tree.png}
       \caption{Phylogenetic tree of the languages in our sample.}
    \label{fig:tree}
\end{figure}




\subsection{Dating Inner Nodes}
We labeled interior nodes for the time at which they split into descendants, using estimates based on historical evidence and the linguistic literature:


%Dates of split are not always strictly meaningful (e.g. for Romance).



\begin{longtable}{llp{10cm}lll}
Group & Split & Source or Rationale \\ \hline
Afroasiatic & 10,000 BC & \cite{diakonoff1998the} \\
Arabic & 1100 AD & Calibration from \citep{holman2011automated} based on end of Arabic domination of Malta. \\
Atlantic-Congo & 4,500 BC & \citet{holman2011automated} estimate an age of 6525 years.\\
Balto-Slavic & 1,400 BC & \citep{gray2003language} \\
Brythonic & 500 AD & Migrations from Britain to Brittany (Humphreys 1993:609. Breton language its present position and historical background, cited by holman2011automated)\\
Central-Semitic & 2,450 BC & \citep{kitchen2009bayesian}   \\
Common Turkic & 700AD & \cite[p. 49]{savelyev2020bayesian} estimate Common Turkic to have split around 474 AD. However, in their model, Old Turkic split off around 650 AD, earlier than the languages in our dataset, with uncertainty about the time of split of the remaining Common Turkic languages. It should predate the earliest documentation of Karluk Middle Turkic after 900AD. We thus put the divergence of the other Common Turkic languages at 700AD. \\
Eastern Baltic & 600 AD & Split between Katvian and Lithuanian \citep[p. 209]{novotna2011glottochronology}\\
Finnic & 800 AD & \cite[Section 4.1]{maurits2020best} \\
Germanic & 250AD & \cite{gray2003language} \\
Global Dutch & 1,600 AD & Dutch colony in South Africa \\
Goidelic & 950 AD & Migrations from Ireland to Scotland. \citet{holman2011automated}, citing \citet{jackson1951gaelic}, calibrates the divergence between Irish and Scottish Gaelic to 950 AD. \\
Hindustani & 1,800 AD & Standardization of Hindi and Urdu\\
Iberian Romance & 1,000 AD & Expansion of Christian kingdoms in Iberia, earliest Iberian Romance texts \\
Icelandic-Faroese & 1,400 AD & Sound shifts specific to Faroese\\
Indo-European & 5,300 BC & \citep{gray2003language} (excluding Hittite and Tocharian, for which we not do not have data). \\
Indo-Iranian & 2,500 BC & \citep[p. 138]{parpola2013formation} \\ % Parpola 1999 suggests 2000 BC. The formation of the Aryan branch of Indo-European.
Insular Celtic & 900BC & \citep{gray2003language} estimate 900BC. Sound shift k$^w$ $>$ p in the Brythonic branch antedates attestation of name `Britain' in 325BC. \\ % Gray and Atkinson: 900BC. Language-tree divergence times support.. Rexova et al, Cladistic analysis
Iranian & 500 BC & \citep{gray2003language}. (Parpola 1999:200) suggests 1900BC based on archaeological evidence. \\ % Kurmanji, Persian. Parpola 1999 suggests 1900 BC.
Italo-Western-Romance & 500 AD & End of the Western Roman empire \citep{holman2011automated}.  \\
Niger-Congo & 5000BC & \citet{holman2011automated} estimate an age of 6227 years, but the family has to be older than Atlantic-Congo. We place Niger-Congo at 5000BC.\\
North-Germanic & 650 AD & Split of Old Norse into regional variants, such as assimilation of nasals to following stops in Western Norse in the 7th century \citep[p. 1856, 1859]{sandoy2017202}. Similarly \citep{holman2011automated} calibrates this to 900 AD. \\% (e.g. assimilation of nasals to following stop in Western Norse in the 7th century. Bandle 2005, Ch. XVII par. 202, The typological ...I Phonology Old East Nordic, p. 1856, 1859. \\
Semitic & 3,750 BC & \citep{kitchen2009bayesian} \\
Serbo-Croatian & 1,900 AD & Standardization of Serbian and Croatian\\
Slavic       & 700AD & \citep{gray2003language}. \citet[p. 209]{novotna2011glottochronology} date the split of East Slavic to the 6th century, \citep{holman2011automated} calibrates it to 550AD. \\
South-Slavic & 750 BC & Expansion of Slavic into Balkan. Postdates Slavic and antedates Old Church Slavonic (attested after 800AD) \\
Uralic & 3,000 BC & \citep[Section 4.7]{maurits2020best}, cf \citep[p. 144]{parpola2013formation} for references \\
West Iberian & 1,100 & Independence of Portugal \\
West-Germanic & 500 AD & Migrations into Britain and southern central Europe\\
West-Scandinavian & 1,100 AD & Sound shifts specific to Norwegian\\
West-Semitic & 3,400 BC & \citep{kitchen2009bayesian}  \\
West-Slavic & 750 BC & Expansion of Slavic. \\ %\citet{holman2011automated} calibrates the split between Czech and Slovak at 1050AD.  \\ % , citing Fodor (1962:132),
Western Romance & 800 AD & Expansion of Christian kingdoms into Iberia \\
Western South Slavic & 1,000 AD & Antedates earliest Slovenian and Serbo-Croatian texts\\
\end{longtable}





\section{Details for Phylogenetic Models}



%We adopt the model of random walks on phylogenetic trees.
%We model the development of grammar and usage as an Ornstein-Uhlenbeck process (See SI Section X for results with a simpler Brownian motion model).
%According to the model of random walks on phylogenetic trees, whenever a (historical) language branches off into multiple successor languages, they independently develop according to this process.


%As the data for optimized grammars comes in the form of counts, we model the usage of a language $L$ as a latent variable $X_L$ that defines the log-odds that an optimized grammar has a subject-object symmetry of 1 (as opposed to 0).
%We model the state of the grammar as $Y_L$, the observed subject-object symmetry.
%$X_L$ and $Y_L$ together form the state $\xi_L \in \mathbb{R} \times [0,1]$  of the language.

%\subsection{Simple Brownian Motion Model}


%We model the instantaneous change of the state $\xi_t$ a language at a given time $t$ as a correlated Brownian motion in two dimensions:
%\begin{equation*}
%    d\xi_t = \Lambda dB_t
%\end{equation*}
%where $\Lambda \in \mathbb{R}^{2\times 2}$ is non-degenerate.

%\subsection{Model with Drift}

%(See below for likelihood results, showing poor fit of the simple Brownian model).


%\section{Details for Phylogenetic Models}



%We adopt the model of random walks on phylogenetic trees.
%We model the development of grammar and usage as an Ornstein-Uhlenbeck process (See SI Section X for results with a simpler Brownian motion model).
%According to the model of random walks on phylogenetic trees, whenever a (historical) language branches off into multiple successor languages, they independently develop according to this process.


%As the data for optimized grammars comes in the form of counts, we model the usage of a language $L$ as a latent variable $X_L$ that defines the log-odds that an optimized grammar has a subject-object symmetry of 1 (as opposed to 0).
%We model the state of the grammar as $Y_L$, the observed subject-object symmetry.
%$X_L$ and $Y_L$ together form the state $\xi_L \in \mathbb{R} \times [0,1]$  of the language.

%\subsection{Simple Brownian Motion Model}


%We model the instantaneous change of the state $\xi_t$ a language at a given time $t$ as a correlated Brownian motion in two dimensions:
%\begin{equation*}
%    d\xi_t = \Lambda dB_t
%\end{equation*}
%where $\Lambda \in \mathbb{R}^{2\times 2}$ is non-degenerate.

%\subsection{Model with Drift}


%This is expressed by the following stochastic differential equation, known as the Ornstein-Uhlenbeck process:
%We model the instantaneous change of the state $\xi_t$ a language at a given time $t$ as a combination of drift and Brownian motion:
%\begin{equation*}
%    d\xi_t = -A(\xi_t-\mu) + \Lambda dB_t
%\end{equation*}
%where $A, \Lambda \in \mathbb{R}^{2\times 2}$, all eigenvalues of $A$ have positive real part, and $\Lambda$ has full rank.


%The matrix $A$ encodes the direction of drift.
%$A_{1,2}$ encodes the extent to which drift of $X_t$ and $Y_t$ is correlated:
%$A_{1,2} > 0$ means that languages drift in such a way that $X_L, Y_L$ are positively correlated (coadaptation); $A_{0,0} = 0$ means that languages do not drift towards a correlation (See Si Section X for an alternative formulation in terms of correlated Brownian motion).


%We model the instantaneous change of the state $\xi_t \in \mathbb{R}^2$ of a language at a given time $t$ as a combination of drift and Brownian motion, formalized by the following stochastic differential equation defining the Ornstein-Uhlenbeck process \citep[p. 109, eq. 4.4.42]{gardiner1983handbook}:
%\begin{equation*}
%    d\xi_t = -\Gamma \cdot (\xi_t-\mu) dt + \Lambda \cdot dB_t
%\end{equation*}
%where $\mu \in \mathbb{R}^2$, $\Gamma, \Lambda \in \mathbb{R}^{2\times 2}$, the real part of each eigenvalue of $\Gamma$ is positive, and $\Lambda$ has full rank (so that $\Lambda\Lambda^T$ is positive definite).
%is positive definite, and $\Lambda$ is diagonal with positive entries (see SI Section for results with non-diagonal $\Lambda$).


%The matrix $A$ encodes the direction of drift.
%$A_{1,2}$ encodes the extent to which drift of $X_t$ and $Y_t$ is correlated:
%$A_{1,2} > 0$ means that languages drift in such a way that $X_L, Y_L$ are positively correlated (coadaptation); $A_{0,0} = 0$ means that languages do not drift towards a correlation (See Si Section X for an alternative formulation in terms of correlated Brownian motion).


%We model the instantaneous change of the state $\xi_t \in \mathbb{R}^2$ of a language at a given time $t$ as a combination of drift and Brownian motion, formalized by the following stochastic differential equation defining the Ornstein-Uhlenbeck process \citep[p. 109, eq. 4.4.42]{gardiner1983handbook}:
%\begin{equation*}
 %   d\xi_t = -\Gamma \cdot (\xi_t-\mu) dt + \Lambda \cdot dB_t
%\end{equation*}
%where $\mu \in \mathbb{R}^2$, $\Gamma, \Lambda \in \mathbb{R}^{2\times 2}$, the real part of each eigenvalue of $\Gamma$ is positive, and $\Lambda$ has full rank (so that $\Lambda\Lambda^T$ is positive definite).
%is positive definite, and $\Lambda$ is diagonal with positive entries (see SI Section for results with non-diagonal $\Lambda$).


%The matrix $A$ encodes the direction of drift.
%$A_{1,2}$ encodes the extent to which drift of $X_t$ and $Y_t$ is correlated:
%$A_{1,2} > 0$ means that languages drift in such a way that $X_L, Y_L$ are positively correlated (coadaptation); $A_{0,0} = 0$ means that languages do not drift towards a correlation (See Si Section X for an alternative formulation in terms of correlated Brownian motion).

%, for comparison with a simple Brownian motion model, and for comparison with non-diagonal $\Gamma$.
%We adopt the model of Brownian motion on phylogenetic trees.
%As the data for optimized grammars comes in the form of counts, we model the usage of a language $L$ as a latent variable $X_L$ that defines the log-odds that an optimized grammar has a subject-object symmetry of 1 (as opposed to 0).
%We model the state of the grammar as $Y_L$, the observed subject-object symmetry.
%$X_L$ and $Y_L$ together form the state $\xi_L \in \mathbb{R} \times [0,1]$  of the language.
%We model the instantaneous change of the state $\xi_t$ a language at a given time $t$ as Brownian motion:
%\begin{equation*}
%    d\xi_t = \Lambda dB_t
%\end{equation*}
%where $\Lambda \in \mathbb{R}^{2\times 2}$.
%$\Lambda_{1,2}$ encodes the extent to which drift of $X_t$ and $Y_t$ is correlated:
%$A_{1,2} > 0$ means that languages drift in such a way that $X_L, Y_L$ are positively correlated (coadaptation); $A_{0,0} = 0$ means that movement in both directions is not correlated.

%The matrix $\Sigma := \Lambda\Lambda^T$ indicates the variance-covariance structure of the instantaneous changes at any time $t$.
%The main quantity of interest is the correlation between changes in the two dimensions, which is given by
%\begin{equation}
%R := \frac{\Sigma_{1,2}}{\sqrt{\Sigma_{1,1}\Sigma_{2,2}}}
%\end{equation}
%A positive value indicates that changes in both directions are positively correlated.

\subsection{Calculating the Likelihood}
For completeness, we describe how to calculate the likelihood of a multidimensional Ornstein-Uhlenbeck model on phylogenetic trees.

As described in the Methods section, it is described by the following stochastic differential equation for the instantaneous change of the state $\xi_{L,t}$ of a language $L$ at a given time $t$:
\begin{equation*}
    \operatorname{d}\xi_{L,t} = \Gamma \cdot (\xi_{L,t}-\mu) \operatorname{d}t + \Lambda \operatorname{d}B_t
\end{equation*}
where $\mu \in \mathbb{R}^2$,  $\Gamma, \Lambda \in \mathbb{R}^{2\times 2}$ are non-degenerate matrices, and $B_t$ is Brownian motion in two dimensions.

In our model, $\Gamma$ is diagonal with positive entries; we found no improved model fit when taking more general choices for $\Gamma$.

The conditional distribution of a future observation at time $t+\Delta$ given an earlier one at time $t$ is given by the following equation \citep[Theorem 3.3]{schach1971weak}, \citep{gardiner1983handbook}, \citep[p. 156, eq. 6.124]{risken1989fokker}:
\begin{equation}
\xi_{L,t+\Delta} | \xi_{L,t} \sim N\left(\mu + e^{-\Delta \Gamma} (\xi_{L,t}-\mu),\ \Omega - e^{-\Delta \Gamma} \Omega e^{-\Delta \Gamma^T}\right)
\end{equation}
where the matrix $\Omega \in \mathbb{R}^{2\times 2}$ is obtained as the solution of the equation \citep[p. 110, eq. 4.4.51]{gardiner1983handbook} \citep[p. 156, eq. 6.126]{risken1989fokker}:
\footnote{Setting $\Sigma := \Lambda\Lambda^T$, this can be solved as follows:
\begin{equation}
\left(\begin{matrix} \Omega_{11} \\ \Omega_{12} \\ \Omega_{22} \end{matrix}\right)=    \left(\begin{matrix}
    2\Gamma_{11} & 2\Gamma_{12} & 0 \\
    \Gamma_{21} & \Gamma_{11}+\Gamma_{22} & \Gamma_{12} \\
    0 & 2\Gamma_{21} & 2\Gamma_{22}
    \end{matrix}\right)^{-1}  \left(\begin{matrix} \Sigma_{11} \\ \Sigma_{12} \\ \Sigma_{22} \end{matrix}\right)
\end{equation}
}
\begin{equation}
    \Gamma\Omega+\Omega\Gamma^T = \Lambda \Lambda^T
\end{equation}
Based on this, one can compute the stationary distribution that solves the differential equation.
The stationary distribution of an individual observation is
\begin{equation}\label{eq:ornuhl-var}
\xi_{t} \sim N\left(\mu, \Omega \right)
\end{equation}
The stationary cross-covariance between the states of two languages $L_1, L_2$, possibly on different branches of the phylogenetic tree, is given by
\begin{equation}\label{eq:ornuhl-covar}
Cov(\xi_{L_1}, \xi_{L_2}) = e^{-\Delta_1 \Gamma} \Omega e^{-\Delta_2 \Gamma^T}
\end{equation}
if $\Delta_1, \Delta_2$ are the times of evolution from their last common ancestor to $L_1$ and $L_2$, respectively.
\footnote{This can be shown as follows:
If $\xi_A$ is the last common ancestor, then (we set $\mu=0$ without loss of generality, as it does not affect the covariance):
\begin{align*}
Cov(\xi_{L_1}, \xi_{L_2}) &= \mathbb{E} \left[\xi_{L_1} \xi_{L_2}^T\right] - \mathbb{E}\xi_{L_1} \mathbb{E}\xi_{L_2}^T   &= \mathbb{E}\left[\mathbb{E} \left[\xi_{L_1} \xi_{L_2}^T | \xi_A\right]\right] - 0 \cdot 0  &= \mathbb{E}\left[\mathbb{E} \left[\xi_{L_1}|\xi_A\right] \mathbb{E} \left[\xi_{L_2}^T | \xi_A\right]\right]  
 &= \mathbb{E}\left[   e^{-\Delta_1\Gamma} \xi_A    \xi_A^T e^{-\Delta_2\Gamma^T} \right]  \\
 &= e^{-\Delta_1\Gamma} \Omega e^{-\Delta_2\Gamma^T}   \\
\end{align*}}
If $L_1, L_2$ do not share a common ancestor (the root in Figure~\ref{fig:tree} does not count as an ancestor), the covariance is zero.\footnote{As $\lim_{\Delta \rightarrow \infty} e^{-\Delta B} = 0$, this is practically equivalent to assuming a very large time-depth of the last common ancestor, which would be the case under the assumption of macrofamilies with very large time depth.}



Since any Ornstein-Uhlenbeck process is Gaussian \citep{schach1971weak}, the joint distribution of any set of observations $\xi_{L, t}$ is determined by (\ref{eq:ornuhl-var}-\ref{eq:ornuhl-covar}).

\subsection{Implementation}

We defined the following priors on the parameters.
We parameterized $\Lambda$ as $D U$, where $U$ is a lower-diagonal matrix, and $D$ is a diagonal matrix.
This is equivalent to modeling $\Sigma := \Lambda\Lambda^T$ as the combination of a correlation matrix and a vector of standard deviations \citep{barnard2000modeling}.
Analogously, we parameterized $\Gamma := E E^T$, where $E$ is also a diagonal matrix.

To define a prior over $\Lambda$, we modeled $U$ as the lower Cholesky factor of a correlation matrix subject to an LKJ(1) prior (\citet{lewandowski2009generating}, i.e., the uniform distribution over $2\times 2$-correlation matrices).
We placed a standard normal prior $N(0,1)$ on the entries of $\mu$ and on the non-zero entries of $D, E$.

We implemented the models in Stan~\citep{carpenter2017stan} and obtained posterior samples using the No-U-Turn sampler.
We ran four chains with 2000 iterations each, of which the first 1000 were discarded as warmup samples.

We computed marginal likelihoods using Stepping Stone Sampling \citep{xie2011improving} with $K=10$ stones.
The fact that $Y_L$ is a latent variable complicates computation of the likelihood; we approximated this by fixing $Y_L$ to the log-odds of the observed empirical counts, adding $1$ to each category to smooth out cases where $Y_L =0,1$.
We verified stability of the estimates by running the procedure ten times for each model, and averaging the obtained marginal likelihoods.


\subsection{Quantifying Instantaneous Correlation}
The matrix $\Sigma := \Lambda\Lambda^T$ indicates the variance-covariance structure of the instantaneous changes at any time $t$ \citep{felsenstein1973maximum, freckleton2012fast}.
The main quantity of interest is the correlation between changes in the two dimensions, which is given by
\begin{equation}
R := \frac{\Sigma_{1,2}}{\sqrt{\Sigma_{1,1}\Sigma_{2,2}}}
\end{equation}
A positive value indicates that changes in both directions are positively correlated.

%\paragraph{Computing Likelihood for Brownian Motion Model}

%We impose an LKJ prior on the correlation matrix of $A$, a student $t$ prior on the variance components of $A$ and $\Lambda$, a ... prior on the two components of $\Lambda$.

%\paragraph{Computing Likelihood for Ornstein-Uhlenbeck Model}

\section{Comparison with Simple Brownian Model}

The simple Brownian model leaves out the drift term, leading to the stochastic differential equation:
\begin{equation*}
    \operatorname{d}\xi_t = \Lambda \operatorname{d}B_t
\end{equation*}
This is known as the Independent Contrasts model \citep{felsenstein1973maximum, freckleton2012fast}.

Brownian motion differs from the Ornstein-Uhlenbeck process in that it does not have a long-term stationary solution.
Instead, trajectories $\xi_t$ tend to move arbitrarily far away from the origin over time $t$.
This is clearly unrealistic in our setting, as subject-object symmetry is bounded between 0 and 1.

As there is no stationary solution, there is no straightforward way to jointly apply the model to data from languages that do not share a common ancestor.
For modelling purposes, we assumed that all families had a common ancestor at some large time $T_0$ in the past.
This modelling assumption corresponds to the assumption of macro-families of very large time-depth.
We varied $T_0$ from 10,000 BC to 100,000 BC, and measured the instantaneous correlation of changes $R$ for each fit.
To evaluate model fit, we compared marginal likelihood of the Brownian model with the Ornstein-Uhlenbeck model.

\paragraph{Results}
Across different choices of $T_0$, the Brownian model strongly supports a positive correlation $R$, very similar to the Ornstein-Uhlenbeck analysis.

TODO results

Model fit as measured by marginal likelihood is considerably weaker in the Brownian model, across choices of $T_0$ (Bayes Factor TODO).

TODO results

%This limitation can be addressed by adding a drift term, whereby languages tend to stay within a bounded area, while subjected to random walk dynamics within this area.


\section{Accounting for Areal Convergence}
The model of random walks on phylogenetic trees assume that languages evolve independently once they have split.
However, linguistic evolution can include borrowing between geographically neighboring languages (CITE).
%\cite{kalyan2019problems}
Here, we provide evidence that our conclusions continue to hold when explicitly modelling convergence in linguistic areas (CITE), geographic regions in which languages tend to show convergent evolution due to borrowing.

%We accounted for effects of geographic neighborhoods by modeling linguistic areas as a latent variable, inferred together with the other parameters of the model.
%We 
We modeled linguistic areas as latent variables defining time- and location-dependent values $\mu(x,t)$ (where $x$ is a point on the surface of the earth and $t$ is a point in time) that languages at time $t$ and place $x$ drift towards.
These values are inferred from the data together with the other parameters of the Ornstein-Uhlenbeck process.
By placing a suitable Gaussian process prior on $\mu(x,t)$, we encourage parameters that smoothly vary over space and time, encoding the idea that areal convergence between languages depends on their geographic distance (CITE).
This approach is related to the model described by \citep{nuismer2015predicting}, who propose to model convergence between species by assuming correlations between the means $\mu$ of different species.
For other approaches to model interactions between species from the bioinformatics literature, see \citet{manceau2016a,drury2016estimating, bartoszek2017using, drury2018an}.


\paragraph{Model}
We model the grammar and usage components of $\mu$ to depend on the language's geographic position and the time a language was spoken.
This models the impact of linguistic areas, and allows this impact to change over time.

We assume that the development of a language $L$ observed at time $t+\Delta$ (e.g. Modern English) developed from a prior state at time $t$ (e.g., Old English) during time $[t, t+\Delta]$ according to the Ornstein-Uhlenbeck SDE
\begin{equation}
    \operatorname{d}\xi_{L,t} = \Gamma \cdot (\xi_{L,t}-\mu_L) \operatorname{d}t + \Lambda \operatorname{d}B_t
\end{equation}
where $\mu_L$ is defined by the temporal and geographical location of the language $L$.


We placed a Gaussian process prior with a Laplace kernel on $\mu$.
That is, the covariance between $\mu$ at points $x, y$ on the surface of the earth at times $T_1, T_2$ was taken to be
\begin{equation}\label{eq:kernel}
    Cov(\mu_x, \mu_y) = \alpha \exp\left(-\frac{1}{\rho^2_1} d(x,y) - \frac{1}{\rho_2^2} |T_1-T_2|\right)
\end{equation}
where $d(x,y)$ is the great circle (geodesic) distance between points $x, y$, and $\alpha, \rho>0$ are hyperparameters.
The Laplace kernel is positive-definite with the great-circle distance $d(\cdot, \cdot)$ \citep{feragen2015geodesic} and thus provides a valid covariance for this distance, unlike some other kernels like the RBF kernel.
This defines a prior that favors values of $\mu_L$ that vary smoothly over space and time, encoding the idea of linguistic areas.
We placed Gaussian priors with mean $0$ and variance $1$, truncated to positive values, on the hyperparameters $\alpha, \frac{1}{\rho^2}$ of the kernel~(\ref{eq:kernel}).


We extracted locations of languages from the World Atlas of Linguistic Structures \citep{haspelmath2005the}.
For ancestors, we recursively defined their location as the mean of the locations of their immediate children.


%As $\mu$ now varies with time, the resulting process is not Gauss-Markov any more \citep{schach1971weak}, and thus the likelihood cannot be represented with a single covariance matrix any more. We thus perform inference by explicitly modelling $\xi_L$ for all inner nodes $L$ of the phylogenetic tree.



\paragraph{Results}
We plot the posterior of the correlation component of $\Sigma$ in Figure~\ref{fig:posterior-area-time}.
The posterior probability that the correlation is $\leq 0$ is estimated to be TODO.

%In Table, we show all models together with the logarithm of the estimated marginal likelihood of the data.
%Higher marginal likelihood indicates better model fit.


%Conditions:
%- $\Gamma$ has positive eigenvalues
%- $\Gamma\Omega+\Omega\Gamma^T$ is a covariance matrix
%- $\Omega$ is a covariance matrix
%Things to visualize
%- stationary distribution
%- instantaneous distribution over change directions


% https://courses.helsinki.fi/sites/default/files/course-material/4523939/Freckleton02.pdf


\section{Subject-Object Symmetry and Categorical Classification of Basic Word Order}


\begin{figure}
    \centering
    \includegraphics[width=0.6\textwidth]{../analysis/categorical_order/figures/by_categorical_order.pdf}
    \caption{Basic word orders as labeled in the World Atlas of Language Structures \citep{wals-81}. Colors indicate basic word order categories, `NA' indicates languages with no data in \citep{wals-81}.}
    \label{fig:categorical-basic-order}
\end{figure}



The notion of subject-object symmetry addresses a key limitation of the standard classification of languages into basic word order categories by accounting for the well-documented differences in degrees of word order flexibilities in languages that are conventionally assigned the same basic word order \citep{steele1978word}.
Here, we compare subject-object symmetry to basic word order as classified in the typological literature.


In Figure~\ref{fig:categorical-basic-order}, we show attested and optimized subject-object symmetry, together with categorical basic word order labels from the World Atlas of Linguistic Structure \citep{wals-81}.
First, we see that attested subject-object symmetry largely determines whether a language is labeled SVO or SOV/VSO.
A few language with medium subject-object symmetry are labeled as `No dominant order'.

Optimized subject-object symmetry remains predictive of attested subject-object order even within the categories SVO ($R = 0.76, p < 10^{-8}$) and SOV ($R=0.55, p = 0.062$).
This shows that the coadaptation theory makes predictions about word order beyond categorical notions of basic word order.


\subsection{Phylogenetic Analysis}

\paragraph{Labeling Categorical Basic Word Order}

We primarily drew on \citep{gell-mann-origin-2011}.

Additional labels for languages in the corpus, without label in \citep{wals-81}:

\begin{longtable}{llllll}
Language & Label & Rationale \\
TODO
\end{longtable}

Labels for unobserved interior nodes:

\begin{longtable}{llllll}
Group & Label & Rationale \\
TODO
\end{longtable}

\paragraph{Model}


\section{The Role of Case Marking and Pro Drop}

\subsection{Case Marking}
Here, we test whether our results can be explained away by assuming that word order and usage patterns independently change in response to the presence or absence of case marking.
We do this by fitting an extension of the model that can model different directions of change in languages with and without case marking, and checking whether the analysis continues to provide evidence for coevolution between word order and usage.

\paragraph{Coding Languages for Case Marking}
We coded languages from our sample for the presence or absence of case marking on the basis of \citep{wals-49}, supplemented with information from the grammatical literature where no information was provided.
We amended the annotation from \citep{wals-49} to include only case marking that distinguishes between subjects and objects; this concerns several modern Celtic and Germanic languages, which have some nominal case marking but do not distinguish subjects and objects (e.g., Swedish and English use -\textit{s} to mark possessives, but do not distinguish nominal subjects and objects.).

We furthermore coded all interior nodes of the phylogenetic tree for case marking based on the linguistic literature.
In some cases, this annotation was unambiguous due to available historical documentation even though no treebank data was available (e.g., Proto-West-Scandinavian was a late form of Old Norse and had case markers).
In many other cases, cognate case markers are unambiguously attested both within and without a group, showing that they were present in the protolanguage (e.g., Proto-Germanic, Proto-Indo-Iranian).
Furthermore, in many protolanguages, case markers are commonly reconstructed based on their presence in different descendant branches (e.g., Proto-Indo-European, Proto-Afroasiatic, Proto-Common-Turkic, Proto-Uralic and Proto-Ugric). % Uraloc: Abondolo Uralic; Hajdu uralischen Sprachen; Marcantonio Uralic
Case is not unambiguously reconstructed for Proto-Niger-Congo and Proto-Atlantic-Congo; we verified that all four possible parameter settings lead to equivalent results (we report results under the assumption that neither of them had case, with essentially indistinguishable results for the other cases).

\paragraph{Model of Change conditioned on Case Marking}
Based on the prior literature, we expect that languages without case marking will be biased towards low subject-object symmetry (CITE).
To take this into account, we modified the model by conditioning the mean vector $\mu$ on the presence or absence of case in the language $L$.
\begin{equation*}
    \operatorname{d}\xi_{L,t} = \Gamma_{C(L)} \cdot (\xi_{L,t}-\mu_{C(L)}) \operatorname{d}t + \Lambda \operatorname{d}B_t
\end{equation*}
where $C(L)$ is $1$ if $L$ has case and $0$ else.
We set priors $\mu_{C(L)} \sim N(0,1)$ for both $C(L) = 0$, and $1$.


\paragraph{Results}
We plot the distribution of languages and the fitted stationary distributions, conditioned on $C(L)$, in Figure~\ref{fig:langs-case}.
In accordance with the prior literature, the model indicated that languages without case marking favor regions with low observed and optimized subject-object symmetry (TODO).
For languages with case marking, there was evidence for a bias towards higher subject-object symmetry (TODO).
We plot the posterior of the correlation component of $\Sigma$ in Figure~\ref{fig:posterior-case} (top).
The posterior probability that the correlation is $\leq 0$ is estimated to be $0.0055$.

To separately estimate the evidence coming from languages with or without case marking, we also fitted a model where $\Lambda$ was also dependent on $C(L)$.
The resulting correlation component posteriors are shown in Figure~\ref{fig:posterior-case} (bottom).
We found a positive correlation in both settings individually (posterior of non-positive correlation $0.04175$ with case, $0.0385$ without case).

\paragraph{Conclusion}
We found that, while case marking has a robust impact on subject-object symmetry, coadaptation continues to hold when controlling for this.

\begin{figure}
    \centering
    \includegraphics[width=0.3\textwidth]{../change/visualize/figures/corr_ornuhl-binom_42.pdf}
    
    \includegraphics[width=0.3\textwidth]{../change/visualize/figures/corr_ornuhl-binom_45_Case.pdf}
    \includegraphics[width=0.3\textwidth]{../change/visualize/figures/corr_ornuhl-binom_45_NoCase.pdf}
    \caption{Posterior of the Usage-Grammar correlation coefficient $R$ when accounting for case marking. Top: Assuming a joint $\Sigma$. Bottom: Assuming separate values for $\Sigma$.}
    \label{fig:posterior-case}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.4\textwidth]{../analysis/figures/by_patient_marking.pdf}
    \includegraphics[width=0.4\textwidth]{../change/visualize/stationary_case.pdf}
    \caption{Left: Languages by availability of morphological distinction between subject and object nouns. Right: Fitted stationary distribution, conditioned on case marking. \mhahn{TODO fix colors}}
    \label{fig:langs-case}
\end{figure}

%#Oliver A. Iggesen. 2013. Number of Cases.
%#In: Dryer, Matthew S. & Haspelmath, Martin (eds.)
%#The World Atlas of Language Structures Online.
%#Leipzig: Max Planck Institute for Evolutionary Anthropology.
%#(Available online at http://wals.info/chapter/49, Accessed on 2020-09-20.)


%\section{Pro-Drop}
%This correlation was not accounted for by differences in the availability of pro-drop (see SI Section X).


%Say something specific:
%Furthermore, we specifically considered what distinguishes the tree topologies of Old English from those of Modern English.
%- Old English -- English what distinguishes the tree structures?
%In this respect, Old English patterned more like contemporary Japanese.


\subsection{Pro Drop}
TODO

\subsection{Subject-Verb Agreement}

\section{Within-Language Correlates of Basic Word Order}

Here, we show that basic word order reflects optimization for DLM not only on the level of languages, but also on the level of individual sentences.

%\subsection{Coexpression: VS Order in SVO Languages}
In many SVO languages, certain intransitive subjects can appear after the verb (``along came a dog'' CITE).
This kind of ``intransitive inversion'' has been documented in many languages, including English (CITE), Romance languages (CITE), and Chinese (CITE).
There are also languages whose basic word order is different in transitive and in intransitive clauses \citep{wals-82}; the World Atlas of Language Structures lists 13 languages with transitive SVO and intransitive VS basic word order \citep{wals-81,wals-82}, while it lists no languages with transitive VSO and intransitive SV order.

%- literature
%- Unaccusative inversion
%-- Leonetti, Two Types of postervbal subject (Spanish): Unaccusative inversion (p. 17)
%-- English there inversion

We conjectured that, more generally, the rate of VS order is higher when no object is present than when an object is present.
For each language in our dataset, we collected statistics for all verbs with a subject and conducted the following logistic analysis:

\begin{equation}
\text{SV Order} \sim \text{Object is present}
\end{equation}

A positive effect indicates that presence of an object makes SV order more likely, compared to VS order.
Results are shown below.
As predicted, in most languages where there is variation between SV and VS order, a significant positive effect was observed.

TODO fill in
\begin{longtable}{l|lllllll}
Language & SV Frequency & $\beta$ & $p$ \\ \hline
\input{../collectPerVerbProperties/SV_withinLang/results_analyze_VSOrderWhenNoObject.R.tex}
\end{longtable}

Also Konstanz Universals Archive No 344: If VS is dominant with transitives, it is also dominant with intransitives, citing Kozinsky 1981
%Kozinsky, Isaak (1981). Nekotorye grammaticeskie universalii v podsistemax vyrazenija subjektno-objektnyx otnnosenij. [Some grammatical universals in subsystems of expression of subject-object relations.] Doctoral dissertation, Moskovskij gosudarstvennyj universitet. 



%\subsection{Embedding: VSO in Embedded Clauses, SVO in Main Clauses}
In some predominant VSO languages, SVO is an alternative word order in unembedded clauses, whereas embedded clauses tend to only allow VSO.
This is in accordance with the predictions of DLM, which favors high subject-object symmetry in embedded clauses (see Figure 1B in the main paper).
Examples include relative clauses in Standard Arabic~\citep{alqurashi:2012}, Breton \citep[][p. 80]{timm1988relative}, Ancient Egyptian \citep{gardiner1957egyptian}, Tuareg \citep[Chapter 12.1.2]{heath2005a}.
Conversely, in some SVO languages, embedded clauses show VSO order (Bantu, \citet{demuth1999verb}); Miza (Chadic) has SVO/VOS in main clauses and VOS in embedded clauses~\citep{wals-81}.
However, it is not generally true that VS order is more common in embedded clauses across all languages that have variation in basic word order.
For instance, German and Dutch can have VS in main clauses, but are almost always SV in subordinate clauses; the same holds for Quileute (Chimakuan)~\citep{wals-81}.



%\section{Detailed Diachronic Trajectories}
%-- coexpression
%-- symmetry
%-- case
%- English
%- French?
%- Ancient Greek?

\section{Estimating Usage and Grammar on Disjoint Corpora}


\bibliography{literature}
\bibliographystyle{natbib}



\end{document}






\begin{table}
\begin{tabular}{lllll}
Model & SDE & File & Log-Likelihood & loo \\ \hline
Uncorrelated   & $d\xi_t = \left(\begin{matrix} \sigma_1 & 0 \\ 0 & \sigma_2\end{matrix}\right) dW_t$ & 30.stan & -239\\
Correlated  & $d\xi_t = \left(\begin{matrix} \sigma_1 & \rho_1 \\ \rho_2 & \sigma_2\end{matrix}\right) dW_t$ & 27, 28, 29 & -197, -220, -195 \\
\hline
Correlated Drift & $d\xi_t = - \left(\begin{matrix} \gamma_1 & \gamma_{1,2} \\ \gamma_{2,1} & \gamma_{2,2}\end{matrix}\right) \left(\xi_t-\mu\right) dt + \left(\begin{matrix} \sigma_1 & 0 \\ 0 & \sigma_2\end{matrix}\right) dW_t$ & 28.stan & -297\\
Correlated + Drift & $d\xi_t = - \left(\begin{matrix} \gamma_1 & 0 \\ 0 & \gamma_{2,2}\end{matrix}\right) \left(\xi_t-\mu\right) dt + \left(\begin{matrix} \sigma_1 & \rho_1 \\ \rho_2 & \sigma_2\end{matrix}\right) dW_t$ & 26.stan & -303 \\
Uncorrelated + Drift & $d\xi_t = - \left(\begin{matrix} \gamma_1 & 0 \\ 0 & \gamma_{2,2}\end{matrix}\right) \left(\xi_t-\mu\right) dt + \left(\begin{matrix} \sigma_1 & 0 \\ 0 & \sigma_2\end{matrix}\right) dW_t$ & 25.stan & -343 \\
Correlated Drift & $d\xi_t = - \left(\begin{matrix} \gamma_1 & \gamma_{1,2} \\ \gamma_{2,1} & \gamma_{2,2}\end{matrix}\right) \left(\xi_t-\mu\right) dt + \left(\begin{matrix} \sigma_1 & \rho_1 \\ \rho_2 & \sigma_2\end{matrix}\right) dW_t$ & 27.stan & -297\\
\hline
Correlated  + Areas & $d\xi_t = - \left(\begin{matrix} \gamma_1 & 0 \\ 0 & \gamma_2\end{matrix}\right) \left(\xi_t-\mu(x)\right) dt + \left(\begin{matrix} \sigma_1 & \rho_1 \\ \rho_2 & \sigma_2\end{matrix}\right) dW_t$ & 29, 31, 32, 33 & -61, -69\\
Uncorrelated  + Areas & $d\xi_t = - \left(\begin{matrix} \gamma_1 & 0 \\ 0 & \gamma_2\end{matrix}\right) \left(\xi_t-\mu(x)\right) dt + \left(\begin{matrix} \sigma_1 & 0 \\ 0 & \sigma_2\end{matrix}\right) dW_t$ & 30 & 113\\
\hline
Correlated  + Area(t) & $d\xi_t = - \left(\begin{matrix} \gamma_1 & 0 \\ 0 & \gamma_2\end{matrix}\right) \left(\xi_t-\mu(x)\right) dt + \left(\begin{matrix} \sigma_1 & \rho_1 \\ \rho_2 & \sigma_2\end{matrix}\right) dW_t$ & 35, 36, 38 & -47, -72, -54\\
Uncorrelated  + Area(t) & $d\xi_t = - \left(\begin{matrix} \gamma_1 & 0 \\ 0 & \gamma_2\end{matrix}\right) \left(\xi_t-\mu(x)\right) dt + \left(\begin{matrix} \sigma_1 & 0 \\ 0 & \sigma_2\end{matrix}\right) dW_t$ & 37 & -109\\
\hline
\end{tabular}
\caption{Phylogenetic drift models. For each model, we provide a representation as a stochastic differential equation, and the logarithm of the estimated marginal likelihood.}
\end{table}

\begin{table}[]
    \centering
    \begin{tabular}{c|c}
   Parameter & Prior \\ 
  $\left(\begin{matrix} \sigma_1 & 0 \\ 0 & \sigma_2\end{matrix}\right)$       &  \\
         $\left(\begin{matrix} \sigma_1 & \rho_1 \\ \rho_2 & \sigma_2\end{matrix}\right)$ & 
    \end{tabular}
    \caption{Caption}
    \label{tab:my_label}
\end{table}
