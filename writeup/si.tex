\documentclass[11pt,a4paper]{article}
\usepackage{times,latexsym}
\usepackage{url}
\usepackage[T1]{fontenc}
\usepackage{grffile}

\usepackage{CJKutf8}
\usepackage{fullpage}

\usepackage{tikz-dependency}

\newcommand\BibTeX{B{\sc ib}\TeX}
\newcommand\confname{EMNLP-IJCNLP 2019}
\newcommand\conforg{SIGDAT}

\usepackage{longtable}

\usepackage{amsmath}
\usepackage{tikz-dependency}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator{\E}{\mathop{\mathbb{E}}}

\usepackage{amssymb}% http://ctan.org/pkg/amssymb
\usepackage{pifont}% http://ctan.org/pkg/pifont
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%


\newcommand{\Prob}{\mathbb{P}}%

%\usepackage{pslatex}
%\usepackage{latexsym}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{bm}
\usepackage{graphicx}
\usepackage{tikz}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{url}
%\usepackage[colorinlistoftodos]{todonotes}
\usepackage{rotating}
\usepackage{multirow}



\newcommand{\japanese}[1]{\begin{CJK}{UTF8}{min}#1\end{CJK}}


\usepackage[T1]{fontenc}

\usepackage{pslatex}
%\usepackage{latexsym}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{bm}
\usepackage{graphicx}
\usepackage{tikz}
\usepackage{xcolor}
\usepackage{url}
%\usepackage[colorinlistoftodos]{todonotes}
\usepackage{rotating}
%\usepackage{natbib}
\usepackage{amssymb}



\usepackage{natbib}
\bibliographystyle{plainnat}

\usepackage{amsthm}
 

\allowdisplaybreaks

\newcounter{theorem}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{question}[theorem]{Question}
\newtheorem{example}[theorem]{Example}
\newtheorem{defin}[theorem]{Definition}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{thm}[theorem]{Theorem}


\newcommand{\R}[0]{\mathbb{R}}
\newcommand{\Ff}[0]{\mathcal{F}}
%\newcommand{}[1]{\textbf{#1}}

\renewcommand{\thefigure}{S\arabic{figure}}
\renewcommand{\thetable}{S\arabic{table}}
\renewcommand{\thesection}{S\arabic{section}}


%\setcounter{figure}{1}
%\setcounter{table}{0}
%\setcounter{section}{0}
%
\newcommand{\soft}[1]{}
\newcommand{\nopreview}[1]{}
\newcommand\comment[1]{{\color{red}#1}}
\newcommand\mhahn[1]{{\color{red}(#1)}}

\newcommand{\thetad}[0]{{\theta_d}}
\newcommand{\thetal}[0]{{\theta_{LM}}}
\newcommand{\thetap}[0]{{\theta_{P}}}


\title{Supplementary Information: Coadaptation between Usage and Grammar in the Evolution of
Word Order across Languages}
% Maybe?: Grammar and Usage Interact in the Evolution of Language

\author{Michael Hahn\\Department of Linguistics, Stanford University\\\\Yang Xu\\ Department of Computer Science, Cognitive Science Program, University of Toronto\\\\ \texttt{mhahn2@stanford.edu}}

\date{}

\begin{document}
\maketitle

\tableofcontents

\section{Historical Languages}

Table~\ref{tab:historical} shows the historical languages in our dataset, with approximate dating assigned.
Table~\ref{tab:traj} lists the languages shown in Figure 4 of the main paper.

\begin{table}
\begin{longtable}{llp{10cm}llll}
Language & Time & Rationale \\ \hline
Archaic Greek & 700 BC & Approximate mean age of texts used \\
Classical Chinese & 300 BC & Life of Mengzi (died around 300 BC); the treebank contains his teachings as collected by his followers. \\
Classical Greek & 400 BC & Approximate mean age of texts used \\
Coptic & 400 AD & Dating of the Apophthegmata Patrum texts used in the UD treebank\\
Gothic & 350 AD & Life of bible translator Ulfilas (311--383)\\
Koine Greek & 0 AD  & Approximate mean age of texts used\\
Latin & 0 AD & Approximate mean age of texts used \\
Old Church Slavonic & 850 AD &  Bible translation after invention of Glagolitic alphabet around 850 AD. \\
Old English & 900 AD & Approximate mean age of texts used \\
Old French & 1200 AD  & Approximate mean age of texts used\\
Old Russian & 1200 AD & Approximate mean age of texts used\\
Sanskrit & 900 BC & Approximate mean age of texts used \\
\end{longtable}
	\caption{Historical languages in our dataset.}\label{tab:historical}
\end{table}

\begin{table}
\begin{longtable}{l|lll} \hline
Groups & Languages & Dates \\ \hline\hline
Chinese & Classical Chinese & -400  \\
& Cantonese & +2000\\ 
& Mandarin & +2000 \\ \hline
East Slavic & Old Russian & +1200 \\
& Russian & +2000 \\ \hline
Eastern South Slavic & Old Church Slavonic & +800 \\
& Bulgarian  & +2000\\ \hline
English & Old English & +900 \\
& English  & +2000\\ \hline
Greek & Archaic Greek & -700 \\
      & Classical Greek & -400 \\
      & Koine Greek & +0\\
& Greek  & +2000\\ \hline
Indo-Aryan & Sanskrit & -900 \\
& Hindi  & +2000\\
& Urdu  & +2000\\ \hline
Romance & Latin &+0  \\
& Old French &+1200\\
& French  & +2000\\
& Italian & +2000\\
& Spanish & +2000\\
& Catalan & +2000\\
& Galician & +2000\\
& Portuguese & +2000\\
& Romanian & +2000\\ \hline
\end{longtable}
	\caption{Languages in Figure 4 of the main paper.}
\label{tab:traj}
\end{table}

\subsection{Corpora and Corpus Sizes}

As described in Methods, we included all UD 2.6 languages with at least 10,000 available words, and the ISWOC Old English treebank.
Table~\ref{tab:sizes} shows the corpus sizes for the included UD languages.
Table~\ref{tab:excluded} shows the UD languages that did not meet the inclusion criterion.
Table~\ref{tab:excluded-corpora} shows excluded treebanks from languages otherwise included.
Table~\ref{tab:spoken-sizes} shows corpus sizes for the spoken corpora (for Naija, see above).



\begin{table}
\begin{tabular}{lrrr}
Language & Number of  & Nunber of \\ 
	& Sentences & Words \\\hline
\input{../fitGrammars/corpora1}
\end{tabular}
\begin{tabular}{lrrr}
Language & Number of  & Nunber of \\ 
	& Sentences & Words \\\hline
\input{../fitGrammars/corpora2}
       &            & \\
\end{tabular}
	\caption{Corpus sizes of the included UD languages.}\label{tab:sizes}
\end{table}


\begin{table}
\begin{longtable}{lrrrr}
Language & Number of  & Nunber of \\ 
	& Sentences & Words \\\hline
\input{../fitGrammars/excluded}
\end{longtable}
	\caption{UD Languages that did not meet the inclusion criterion.}\label{tab:excluded}
\end{table}


\begin{table}
\begin{longtable}{lllll}
Treebank & Rationale \\ \hline
Chinese-CFL & Text written by non-native speakers\\
English-ESL & Text written by non-native speakers \\
English-Pronouns & Specifically targets pronouns \\
French-FQB & Consists entirely of questions \\
Hindi English-HIENCS &  Consists of codeswitched text \\
Latin-ITTB & Consists of Medieval Latin text \\
Latin-LLCT & Consists of Medieval Latin text  \\ 
\end{longtable}
	\caption{UD corpora excluded, from languages otherwise excluded.}\label{tab:excluded-corpora}
\end{table}



\begin{table}
\begin{longtable}{lrrrr}
Language & Number of Sentences  \\ \hline
\input{../optimizeDLM/Switchboard/corpora}
\input{../optimizeDLM/UD_Spoken/corpora}
\input{../optimizeDLM/SpokenJapanese/corpora}
\end{longtable}
	\caption{Corpus sizes for spoken corpora.}\label{tab:spoken-sizes}
\end{table}

\section{Phylogenetic Tree}

\subsection{Tree Topology}

We obtained tree topologies from Glottolog~\citep{nordhoff2011glottolog}.
We only retained interior nodes when more than one of their daughter nodes had languages in our dataset.
The resulting tree topology is displayed in Figure~\ref{fig:tree}.\footnote{Tree obtained with https://icytree.org/.}


\begin{figure}
    \centering
	\rotatebox{270}{\includegraphics[width=1.3\textwidth]{../trees/tree.png}}
       \caption{Phylogenetic tree topology of the languages in our sample. Compare Figure~\ref{fig:tree-times} for a version indicating the time depth of different families.}
    \label{fig:tree}
\end{figure}


\begin{figure}
    \centering
	\rotatebox{270}{\includegraphics[width=1.3\textwidth]{../trees/tree-times.png}}
	\caption{Phylogenetic tree of the languages in our sample. The length of branches reflects distance in time. Compare Figure~\ref{fig:tree} for a version indicating the raw topology without time depths.}
    \label{fig:tree-times}
\end{figure}




\subsection{Dating Inner Nodes}
We labeled interior nodes for the time at which they split into descendants, using estimates based on historical evidence and the linguistic literature:


%Dates of split are not always strictly meaningful (e.g. for Romance).



\begin{longtable}{llp{10cm}lll}
Group & Split & Source or Rationale \\ \hline
Afroasiatic & 10,000 BC & \cite{diakonoff1998the} \\
Arabic & 1,100 AD & Calibration from \citep{holman2011automated} based on end of Arabic domination of Malta. \\
Atlantic-Congo & 4,500 BC & \citet{holman2011automated} estimate an age of 6525 years.\\
Balto-Slavic & 1,400 BC & \citep{gray2003language} \\
	Brythonic & 500 AD & Migrations from Britain to Brittany \citep{holman2011automated}\\ % (Humphreys 1993:609. Breton language its present position and historical background, cited by 
Central-Semitic & 2,450 BC & \citep{kitchen2009bayesian}   \\
Common Turkic & 700AD & \cite[p. 49]{savelyev2020bayesian} estimate Common Turkic to have split around 474 AD. However, in their model, Old Turkic split off around 650 AD, earlier than the languages in our dataset, with uncertainty about the time of split of the remaining Common Turkic languages. It should predate the earliest documentation of Karluk Middle Turkic after 900AD. We thus put the divergence of the other Common Turkic languages at 700AD. \\
Eastern Baltic & 600 AD & Split between Katvian and Lithuanian \citep[p. 209]{novotna2011glottochronology}\\
Finnic & 800 AD & \cite[Section 4.1]{maurits2020best} \\
Germanic & 250AD & \cite{gray2003language} \\
Global Dutch & 1,600 AD & Dutch colony in South Africa \\
Goidelic & 950 AD & Migrations from Ireland to Scotland. \citet{holman2011automated}, citing \citet{jackson1951gaelic}, calibrates the divergence between Irish and Scottish Gaelic to 950 AD. \\
Hindustani & 1,800 AD & Standardization of Hindi and Urdu\\
Iberian Romance & 1,000 AD & Expansion of Christian kingdoms in Iberia, earliest Iberian Romance texts \\
Icelandic-Faroese & 1,400 AD & Sound shifts specific to Faroese\\
Indo-European & 5,300 BC & \citep{gray2003language} (excluding Hittite and Tocharian, for which we not do not have data). \\
Indo-Iranian & 2,500 BC & \citep[p. 138]{parpola2013formation} \\ % Parpola 1999 suggests 2000 BC. The formation of the Aryan branch of Indo-European.
Insular Celtic & 900BC & \citep{gray2003language} estimate 900BC. Sound shift k$^w$ $>$ p in the Brythonic branch antedates attestation of name `Britain' in 325BC. \\ % Gray and Atkinson: 900BC. Language-tree divergence times support.. Rexova et al, Cladistic analysis
Iranian & 500 BC & \citep{gray2003language}. \\ % (Parpola 1999:200) suggests 1900BC based on archaeological evidence. \\ % Kurmanji, Persian. Parpola 1999 suggests 1900 BC.
Italo-Western-Romance & 500 AD & End of the Western Roman empire \citep{holman2011automated}.  \\
	Macro-English & 1900AD & In our dataset, this is the common ancestor of contemporary English and Naija (Nigerian Pidgin). \\
Niger-Congo & 5000BC & \citet{holman2011automated} estimate an age of 6227 years, but the family has to be older than Atlantic-Congo. We place Niger-Congo at 5000BC.\\
North-Germanic & 650 AD & Split of Old Norse into regional variants, such as assimilation of nasals to following stops in Western Norse in the 7th century \citep[p. 1856, 1859]{sandoy2017202}. Similarly \citep{holman2011automated} calibrates this to 900 AD. \\% (e.g. assimilation of nasals to following stop in Western Norse in the 7th century. Bandle 2005, Ch. XVII par. 202, The typological ...I Phonology Old East Nordic, p. 1856, 1859. \\
Semitic & 3,750 BC & \citep{kitchen2009bayesian} \\
Serbo-Croatian & 1,900 AD & Standardization of Serbian and Croatian\\
Slavic       & 700AD & \citep{gray2003language}. \citet[p. 209]{novotna2011glottochronology} date the split of East Slavic to the 6th century, \citep{holman2011automated} calibrates it to 550AD. \\
South-Slavic & 750 BC & Expansion of Slavic into Balkan. Postdates Slavic and antedates Old Church Slavonic (attested after 800AD) \\
Uralic & 3,000 BC & \citep[Section 4.7]{maurits2020best}, cf \citep[p. 144]{parpola2013formation} for references \\
West Iberian & 1,100AD & Independence of Portugal \\
West-Germanic & 500 AD & Migrations into Britain and southern central Europe\\
West-Scandinavian & 1,100 AD & Sound shifts specific to Norwegian\\
West-Semitic & 3,400 BC & \citep{kitchen2009bayesian}  \\
West-Slavic & 750 BC & Expansion of Slavic. \\ %\citet{holman2011automated} calibrates the split between Czech and Slovak at 1050AD.  \\ % , citing Fodor (1962:132),
Western Romance & 800 AD & Expansion of Christian kingdoms into Iberia \\
Western South Slavic & 1,000 AD & Antedates earliest Slovenian and Serbo-Croatian texts\\
\end{longtable}





\section{Details for Phylogenetic Models}



%We adopt the model of random walks on phylogenetic trees.
%We model the development of grammar and usage as an Ornstein-Uhlenbeck process (See SI Section X for results with a simpler Brownian motion model).
%According to the model of random walks on phylogenetic trees, whenever a (historical) language branches off into multiple successor languages, they independently develop according to this process.


%As the data for optimized grammars comes in the form of counts, we model the usage of a language $L$ as a latent variable $X_L$ that defines the log-odds that an optimized grammar has a subject-object position congruence of 1 (as opposed to 0).
%We model the state of the grammar as $Y_L$, the observed subject-object position congruence.
%$X_L$ and $Y_L$ together form the state $\xi_L \in \mathbb{R} \times [0,1]$  of the language.

%\subsection{Simple Brownian Motion Model}


%We model the instantaneous change of the state $\xi_t$ a language at a given time $t$ as a correlated Brownian motion in two dimensions:
%\begin{equation*}
%    d\xi_t = \Lambda dB_t
%\end{equation*}
%where $\Lambda \in \mathbb{R}^{2\times 2}$ is non-degenerate.

%\subsection{Model with Drift}

%(See below for likelihood results, showing poor fit of the simple Brownian model).


%\section{Details for Phylogenetic Models}



%We adopt the model of random walks on phylogenetic trees.
%We model the development of grammar and usage as an Ornstein-Uhlenbeck process (See SI Section X for results with a simpler Brownian motion model).
%According to the model of random walks on phylogenetic trees, whenever a (historical) language branches off into multiple successor languages, they independently develop according to this process.


%As the data for optimized grammars comes in the form of counts, we model the usage of a language $L$ as a latent variable $X_L$ that defines the log-odds that an optimized grammar has a subject-object position congruence of 1 (as opposed to 0).
%We model the state of the grammar as $Y_L$, the observed subject-object position congruence.
%$X_L$ and $Y_L$ together form the state $\xi_L \in \mathbb{R} \times [0,1]$  of the language.

%\subsection{Simple Brownian Motion Model}


%We model the instantaneous change of the state $\xi_t$ a language at a given time $t$ as a correlated Brownian motion in two dimensions:
%\begin{equation*}
%    d\xi_t = \Lambda dB_t
%\end{equation*}
%where $\Lambda \in \mathbb{R}^{2\times 2}$ is non-degenerate.

%\subsection{Model with Drift}


%This is expressed by the following stochastic differential equation, known as the Ornstein-Uhlenbeck process:
%We model the instantaneous change of the state $\xi_t$ a language at a given time $t$ as a combination of drift and Brownian motion:
%\begin{equation*}
%    d\xi_t = -A(\xi_t-\mu) + \Lambda dB_t
%\end{equation*}
%where $A, \Lambda \in \mathbb{R}^{2\times 2}$, all eigenvalues of $A$ have positive real part, and $\Lambda$ has full rank.


%The matrix $A$ encodes the direction of drift.
%$A_{1,2}$ encodes the extent to which drift of $X_t$ and $Y_t$ is correlated:
%$A_{1,2} > 0$ means that languages drift in such a way that $X_L, Y_L$ are positively correlated (coadaptation); $A_{0,0} = 0$ means that languages do not drift towards a correlation (See Si Section X for an alternative formulation in terms of correlated Brownian motion).


%We model the instantaneous change of the state $\xi_t \in \mathbb{R}^2$ of a language at a given time $t$ as a combination of drift and Brownian motion, formalized by the following stochastic differential equation defining the Ornstein-Uhlenbeck process \citep[p. 109, eq. 4.4.42]{gardiner1983handbook}:
%\begin{equation*}
%    d\xi_t = -\Gamma \cdot (\xi_t-\mu) dt + \Lambda \cdot dB_t
%\end{equation*}
%where $\mu \in \mathbb{R}^2$, $\Gamma, \Lambda \in \mathbb{R}^{2\times 2}$, the real part of each eigenvalue of $\Gamma$ is positive, and $\Lambda$ has full rank (so that $\Lambda\Lambda^T$ is positive definite).
%is positive definite, and $\Lambda$ is diagonal with positive entries (see SI Section for results with non-diagonal $\Lambda$).


%The matrix $A$ encodes the direction of drift.
%$A_{1,2}$ encodes the extent to which drift of $X_t$ and $Y_t$ is correlated:
%$A_{1,2} > 0$ means that languages drift in such a way that $X_L, Y_L$ are positively correlated (coadaptation); $A_{0,0} = 0$ means that languages do not drift towards a correlation (See Si Section X for an alternative formulation in terms of correlated Brownian motion).


%We model the instantaneous change of the state $\xi_t \in \mathbb{R}^2$ of a language at a given time $t$ as a combination of drift and Brownian motion, formalized by the following stochastic differential equation defining the Ornstein-Uhlenbeck process \citep[p. 109, eq. 4.4.42]{gardiner1983handbook}:
%\begin{equation*}
 %   d\xi_t = -\Gamma \cdot (\xi_t-\mu) dt + \Lambda \cdot dB_t
%\end{equation*}
%where $\mu \in \mathbb{R}^2$, $\Gamma, \Lambda \in \mathbb{R}^{2\times 2}$, the real part of each eigenvalue of $\Gamma$ is positive, and $\Lambda$ has full rank (so that $\Lambda\Lambda^T$ is positive definite).
%is positive definite, and $\Lambda$ is diagonal with positive entries (see SI Section for results with non-diagonal $\Lambda$).


%The matrix $A$ encodes the direction of drift.
%$A_{1,2}$ encodes the extent to which drift of $X_t$ and $Y_t$ is correlated:
%$A_{1,2} > 0$ means that languages drift in such a way that $X_L, Y_L$ are positively correlated (coadaptation); $A_{0,0} = 0$ means that languages do not drift towards a correlation (See Si Section X for an alternative formulation in terms of correlated Brownian motion).

%, for comparison with a simple Brownian motion model, and for comparison with non-diagonal $\Gamma$.
%We adopt the model of Brownian motion on phylogenetic trees.
%As the data for optimized grammars comes in the form of counts, we model the usage of a language $L$ as a latent variable $X_L$ that defines the log-odds that an optimized grammar has a subject-object position congruence of 1 (as opposed to 0).
%We model the state of the grammar as $Y_L$, the observed subject-object position congruence.
%$X_L$ and $Y_L$ together form the state $\xi_L \in \mathbb{R} \times [0,1]$  of the language.
%We model the instantaneous change of the state $\xi_t$ a language at a given time $t$ as Brownian motion:
%\begin{equation*}
%    d\xi_t = \Lambda dB_t
%\end{equation*}
%where $\Lambda \in \mathbb{R}^{2\times 2}$.
%$\Lambda_{1,2}$ encodes the extent to which drift of $X_t$ and $Y_t$ is correlated:
%$A_{1,2} > 0$ means that languages drift in such a way that $X_L, Y_L$ are positively correlated (coadaptation); $A_{0,0} = 0$ means that movement in both directions is not correlated.

%The matrix $\Sigma := \Lambda\Lambda^T$ indicates the variance-covariance structure of the instantaneous changes at any time $t$.
%The main quantity of interest is the correlation between changes in the two dimensions, which is given by
%\begin{equation}
%R := \frac{\Sigma_{1,2}}{\sqrt{\Sigma_{1,1}\Sigma_{2,2}}}
%\end{equation}
%A positive value indicates that changes in both directions are positively correlated.

\subsection{Calculating the Likelihood}
For completeness, we describe how to calculate the likelihood of a multidimensional Ornstein-Uhlenbeck model on phylogenetic trees \citep{felsenstein1988phylogenies,hansen1997stabilizing, blackwell2003bayesian}.

As described in the Methods section, it is described by the following stochastic differential equation for the instantaneous change of the state $\xi_{L,t}$ of a language $L$ at a given time $t$:
\begin{equation*}
    \operatorname{d}\xi_{L,t} = \Gamma \cdot (\xi_{L,t}-\mu) \operatorname{d}t + \Lambda \operatorname{d}B_t
\end{equation*}
where $\mu \in \mathbb{R}^2$,  $\Gamma, \Lambda \in \mathbb{R}^{2\times 2}$ are non-degenerate matrices, and $B_t$ is Brownian motion in two dimensions.

In our model, $\Gamma$ is diagonal with positive entries; we found no improved model fit when taking more general choices for $\Gamma$.

The conditional distribution of a future observation at time $t+\Delta$ given an earlier one at time $t$ is given by the following equation \citep[Theorem 3.3]{schach1971weak}, \citep{gardiner1983handbook}, \citep[p. 156, eq. 6.124]{risken1989fokker}:
\begin{equation}
\xi_{L,t+\Delta} | \xi_{L,t} \sim N\left(\mu + e^{-\Delta \Gamma} (\xi_{L,t}-\mu),\ \Omega - e^{-\Delta \Gamma} \Omega e^{-\Delta \Gamma^T}\right)
\end{equation}
where the matrix $\Omega \in \mathbb{R}^{2\times 2}$ is obtained as the solution of the equation \citep[p. 110, eq. 4.4.51]{gardiner1983handbook} \citep[p. 156, eq. 6.126]{risken1989fokker}:
\footnote{Setting $\Sigma := \Lambda\Lambda^T$, this can be solved as follows:
\begin{equation}
\left(\begin{matrix} \Omega_{11} \\ \Omega_{12} \\ \Omega_{22} \end{matrix}\right)=    \left(\begin{matrix}
    2\Gamma_{11} & 2\Gamma_{12} & 0 \\
    \Gamma_{21} & \Gamma_{11}+\Gamma_{22} & \Gamma_{12} \\
    0 & 2\Gamma_{21} & 2\Gamma_{22}
    \end{matrix}\right)^{-1}  \left(\begin{matrix} \Sigma_{11} \\ \Sigma_{12} \\ \Sigma_{22} \end{matrix}\right)
\end{equation}
}
\begin{equation}
    \Gamma\Omega+\Omega\Gamma^T = \Lambda \Lambda^T
\end{equation}
One can compute the stationary distribution that solves the differential equation as follows.
The stationary distribution of an individual observation is
\begin{equation}\label{eq:ornuhl-var}
\xi_{t} \sim N\left(\mu, \Omega \right)
\end{equation}
The stationary cross-covariance between the states of two languages $L_1, L_2$, possibly on different branches of the phylogenetic tree, is given by
\begin{equation}\label{eq:ornuhl-covar}
Cov(\xi_{L_1}, \xi_{L_2}) = e^{-\Delta_1 \Gamma} \Omega e^{-\Delta_2 \Gamma^T}
\end{equation}
where $\Delta_1, \Delta_2$ are the times of evolution from their last common ancestor to $L_1$ and $L_2$, respectively.
\footnote{This can be shown as follows:
If $\xi_A$ is the last common ancestor, then (we set $\mu=0$ without loss of generality, as it does not affect the covariance):
\begin{align*}
Cov(\xi_{L_1}, \xi_{L_2}) &= \mathbb{E} \left[\xi_{L_1} \xi_{L_2}^T\right] - \mathbb{E}\xi_{L_1} \mathbb{E}\xi_{L_2}^T   &= \mathbb{E}\left[\mathbb{E} \left[\xi_{L_1} \xi_{L_2}^T | \xi_A\right]\right] - 0 \cdot 0  &= \mathbb{E}\left[\mathbb{E} \left[\xi_{L_1}|\xi_A\right] \mathbb{E} \left[\xi_{L_2}^T | \xi_A\right]\right]  
 &= \mathbb{E}\left[   e^{-\Delta_1\Gamma} \xi_A    \xi_A^T e^{-\Delta_2\Gamma^T} \right]  \\
 &= e^{-\Delta_1\Gamma} \Omega e^{-\Delta_2\Gamma^T}   \\
\end{align*}}
If $L_1, L_2$ do not share a common ancestor (the root in Figure~\ref{fig:tree} does not count as an ancestor), the covariance is zero.\footnote{As $\lim_{\Delta \rightarrow \infty} e^{-\Delta B} = 0$, this is practically equivalent to assuming a very large time-depth of the last common ancestor, which would be the case under the assumption of macrofamilies with very large time depth.}



Since any Ornstein-Uhlenbeck process is Gaussian \citep{schach1971weak}, the joint distribution of any set of observations $\xi_{L, t}$ is determined by (\ref{eq:ornuhl-var}-\ref{eq:ornuhl-covar}).

\subsection{Implementation}

We defined the following priors on the parameters.
We parameterized $\Lambda$ as $D U$, where $U$ is a lower-diagonal matrix, and $D$ is a diagonal matrix.
This is equivalent to modeling $\Sigma := \Lambda\Lambda^T$ as the combination of a correlation matrix and a vector of standard deviations \citep{barnard2000modeling}.
Analogously, we parameterized $\Gamma := E E^T$, where $E$ is also a diagonal matrix.

To define a prior over $\Lambda$, we modeled $U$ as the lower Cholesky factor of a correlation matrix subject to an LKJ(1) prior (\citet{lewandowski2009generating}, i.e., the uniform distribution over $2\times 2$-correlation matrices).
We placed a standard normal prior $N(0,1)$ on the entries of $\mu$ and on the non-zero entries of $D, E$.

We implemented the models in Stan~\citep{carpenter2017stan} and obtained posterior samples using the No-U-Turn sampler.
We ran four chains with 2000 iterations each, of which the first 1000 were discarded as warmup samples.

We computed marginal likelihoods using Stepping Stone Sampling \citep{xie2011improving} with $K=10$ stones.
The fact that $Y_L$ is a latent variable complicates computation of the likelihood; we approximated this by fixing $Y_L$ to the log-odds of the observed empirical counts, adding $1$ to each category to smooth out cases where $Y_L =0,1$.
We verified stability of the estimates by running the procedure ten times for each model, and averaging the obtained marginal likelihoods.


\subsection{Correlation Component of $\Sigma$}\label{sec:instant-corr}
In the main analysis, we reported the correlation between the two dimensions in the stationary distribution $\Omega$.
In some of the analyses in the SI, there are multiple stationary distributions (depending on geography, case marking, or categorical order classes, see Sections 5--7).
In these cases, we therefore report correlation for the instantaneous changes at any point in time:
The matrix $\Sigma := \Lambda\Lambda^T$ indicates the variance-covariance structure of the instantaneous changes at any time $t$ \citep{felsenstein1973maximum, freckleton2012fast}.
The main quantity of interest is the correlation between changes in the two dimensions \citep[cf.][]{felsenstein1973maximum,freckleton2012fast}, which is given by
\begin{equation}
R_\Sigma := \frac{\Sigma_{1,2}}{\sqrt{\Sigma_{1,1}\Sigma_{2,2}}}
\end{equation}
A positive value indicates that changes in both directions are positively correlated.
%We show the posterior of $R_\Sigma$ for the main analysis in Figure~\ref{fig:corr-main}.
%
%\begin{figure}
%    \centering
%	\begin{tabular}{cccc}
%    \includegraphics[width=0.5\textwidth]{../change/ornuhl-binom/noLatents/fits/corr_sigma.pdf} \\
%	\end{tabular}
%    \caption{Posterior of the correlation component $R_\Sigma$ (see Section~\ref{sec:instant-corr}) in the main analysis.}
%    \label{fig:corr-main}
%\end{figure}
%


%\paragraph{Computing Likelihood for Brownian Motion Model}

%We impose an LKJ prior on the correlation matrix of $A$, a student $t$ prior on the variance components of $A$ and $\Lambda$, a ... prior on the two components of $\Lambda$.

%\paragraph{Computing Likelihood for Ornstein-Uhlenbeck Model}

\section{Comparison with Simple Brownian Model}

The simple Brownian model leaves out the drift term, leading to the stochastic differential equation:
\begin{equation*}
    \operatorname{d}\xi_t = \Lambda \operatorname{d}B_t
\end{equation*}
This is known as the Independent Contrasts model \citep{felsenstein1973maximum, freckleton2012fast}.

Brownian motion differs from the Ornstein-Uhlenbeck process in that it does not have a long-term stationary solution.
Instead, trajectories $\xi_t$ tend to move arbitrarily far away from the origin over time $t$.
This is clearly unrealistic in our setting, as subject-object position congruence is bounded between 0 and 1.

As there is no stationary solution, there is no straightforward way to jointly apply the model to data from languages that do not share a common ancestor.
For modelling purposes, we assumed that all families had a common ancestor at some large time $T_0$ in the past.
This modelling assumption corresponds to the assumption of macro-families of very large time-depth.
We considered $T_0$ to be 20,000 BC, 50,000 BC, and 100,000 BC, and measured the instantaneous correlation of changes $R$ for each fit.
To evaluate model fit, we compared marginal likelihood of the Brownian model with the Ornstein-Uhlenbeck model.

\paragraph{Results}
Across different choices of $T_0$, the Brownian model strongly supports a positive correlation $R$, very similar to the Ornstein-Uhlenbeck analysis; the posterior probability of $R\leq 0$ is $0.00025$ at $T_0=-100,000$, $0.00025$ at $T_0 = -50,000$, and $0$ at $T_0 = -20,000$. % change/brownian-binom/fits
However, model fit as measured by marginal likelihood is much weaker than in the Ornstein-Uhlenbeck model, across choices of $T_0$ (Table~\ref{tab:marg-brown}), corresponding to a Bayes factor of about $10^{55}$ in favor of the Ornstein-Uhlenbeck model.

\begin{table}
	\begin{center}
	\begin{tabular}{llllllll}
	% change/brownian-binom/logodds-approx/marginal_likelihood$ more results.txt
	Model & Log-Likelihood \\ \hline
	Ornstein-Uhlenbeck & -34 \\
	Ornstein-Uhlenbeck (diaognal $\Sigma$) & -40 \\
	Brownian ($T_0 = -100,000$) & -190 \\
	Brownian ($T_0 = -50,000$) & -188 \\
	Brownian ($T_0 = -20,000$) & -170
\end{tabular}
	\end{center}
	\caption{Marginal log-likelihoods for Ornstein-Uhlenbeck and simple Brownian models. Values closer to $0$ indicate better model fit.}
	\label{tab:marg-brown}
\end{table}

\section{Accounting for Areal Convergence}
The model of random walks on phylogenetic trees assume that languages evolve independently once they have split.
However, linguistic evolution can include borrowing between geographically neighboring languages \citep[e.g.][]{dryer1989large, bisang1996areal, heine2003on, aikhenvald2007grammars,  kalyan2019problems}.

Here, we provide evidence that our conclusions continue to hold when explicitly modelling convergence in linguistic areas, geographic regions in which languages tend to show convergent evolution due to borrowing  \citep[e.g.][]{campbell1986meso, nichols1992linguistic, haspelmath2001the, gijn2017linguistic}.

We modeled linguistic areas as latent variables defining time- and location-dependent values $\mu(x,t)$ (where $x$ is a point on the surface of the earth and $t$ is a point in time) that languages at time $t$ and place $x$ drift towards.
These values are inferred from the data together with the other parameters of the Ornstein-Uhlenbeck process.
By placing a suitable Gaussian process prior on $\mu(x,t)$, we encourage parameters that smoothly vary over space and time, reflecting the idea that areal convergence between languages depends on their geographic distance.
This approach is related to the model described by \citep{nuismer2015predicting}, who propose to model convergence between species by assuming correlations between the means $\mu$ of different species.
For other approaches to model interactions between species from the bioinformatics literature, see \citet{manceau2016a,drury2016estimating, bartoszek2017using, drury2018an}.


\paragraph{Model}
We model the grammar and usage components of $\mu$ to depend on the language's geographic position and the time a language was spoken.
This models the impact of linguistic areas, and allows this impact to change over time.

We assume that a language $L$ observed at time $t+\Delta$ (e.g. Modern English) developed from a prior state at time $t$ (e.g., Old English) during time $[t, t+\Delta]$ according to the Ornstein-Uhlenbeck SDE
\begin{equation}
    \operatorname{d}\xi_{L,t} = \Gamma \cdot (\xi_{L,t}-\mu_L) \operatorname{d}t + \Lambda \operatorname{d}B_t
\end{equation}
where $\mu_L$ is defined by the temporal and geographical location of the language $L$.


We placed a Gaussian process prior with a Laplace kernel on $\mu$.
That is, the covariance between $\mu$ at points $x, y$ on the surface of the earth at times $T_1, T_2$ is taken to be
\begin{equation}\label{eq:kernel}
    Cov(\mu_x, \mu_y) = \alpha \cdot \exp\left(-\frac{1}{\rho^2_1} d(x,y) - \frac{1}{\rho_2^2} |T_1-T_2|\right)
\end{equation}
where $d(x,y)$ is the great circle (geodesic) distance between points $x, y$, and $\alpha, \rho>0$ are hyperparameters.
The Laplace kernel is positive-definite with the great-circle distance $d(\cdot, \cdot)$ \citep{feragen2015geodesic} and thus provides a valid covariance for this distance; many other popular kernels like the RBF kernel are not valid for this distance \citep{feragen2015geodesic}.
This prior favors values of $\mu_L$ that vary smoothly over space and time, encoding the idea of linguistic areas.
We placed Gaussian priors with mean $0$ and variance $1$, truncated to positive values, on the hyperparameters $\alpha, \frac{1}{\rho^2}$ of the kernel~(\ref{eq:kernel}).


We extracted locations of languages from the World Atlas of Linguistic Structures \citep{haspelmath2005the}.
For ancestors, we recursively defined their location as the mean of the locations of their immediate children.


%As $\mu$ now varies with time, the resulting process is not Gauss-Markov any more \citep{schach1971weak}, and thus the likelihood cannot be represented with a single covariance matrix any more. We thus perform inference by explicitly modelling $\xi_L$ for all inner nodes $L$ of the phylogenetic tree.



\paragraph{Results}
We plot the posterior of the correlation component of $\Sigma$ (see Section~\ref{sec:instant-corr}) in Figure~\ref{fig:posterior-area-time}.
The posterior probability that the correlation is $\leq 0$ is estimated to be 0.00125.
In the stationary distribution, the two components are correlated at $R = 0.59$ (95\% credible interval $[0.25, 0.89]$).
This confirms that coadaptation is found even when explicitly modeling convergence between neighboring languages.


\begin{figure}
	\begin{center}
    \includegraphics[width=0.5\textwidth]{../change/ornuhl-binom-geo-time/fits/corr_sigma.pdf}
	\end{center}
	\caption{Posterior of the correlation component of $\Sigma$ (see Section~\ref{sec:instant-corr}) when accounting for areal convergence.}\label{fig:posterior-area-time}
\end{figure}


%In Table, we show all models together with the logarithm of the estimated marginal likelihood of the data.
%Higher marginal likelihood indicates better model fit.


%Conditions:
%- $\Gamma$ has positive eigenvalues
%- $\Gamma\Omega+\Omega\Gamma^T$ is a covariance matrix
%- $\Omega$ is a covariance matrix
%Things to visualize
%- stationary distribution
%- instantaneous distribution over change directions


% https://courses.helsinki.fi/sites/default/files/course-material/4523939/Freckleton02.pdf


\section{Subject-Object Position Congruence and Categorical Classification of Basic Word Order}\label{sec:categorical}


\begin{figure}
    \centering
    \includegraphics[width=0.6\textwidth]{../analysis/categorical_order/figures/by_categorical_order.pdf}
	\caption{Basic word orders as labeled in the World Atlas of Language Structures \citep{wals-81}, supplemented with data from other sources (see Section~\ref{sec:categorical}).}
    \label{fig:categorical-basic-order}
\end{figure}



The notion of subject-object position congruence addresses a key limitation of the standard classification of languages into basic word order categories by accounting for the well-documented differences in degrees of word order flexibilities in languages that are conventionally assigned the same basic word order \citep{steele1978word}.
Here, we compare subject-object position congruence to basic word order as classified in the typological literature.


In Figure~\ref{fig:categorical-basic-order}, we show attested and optimized subject-object position congruence, together with categorical basic word order labels from the World Atlas of Linguistic Structure \citep{wals-81}, supplemented with data from other sources where it had no data (primarily \citep{gell-mann-origin-2011}, see Table~\ref{tab:categorical}).
First, we see that attested subject-object position congruence largely determines whether a language is labeled SVO or SOV/VSO.
A few language with medium subject-object position congruence are labeled as `No dominant order'.

Optimized subject-object position congruence remains predictive of attested subject-object order even within the categories SVO ($R = 0.52, p < 0.00021$) and SOV ($R=0.45, p = 0.07$).
This suggests that the coadaptation theory makes predictions about word order beyond categorical notions of basic word order.

Next, we describe a version of the phylogenetic analysis controlling for catgeorical order.

\subsection{Phylogenetic Analysis}\label{sec:categorical}


\paragraph{Model}

We constructed a version of the phylogenetic analysis where the drift parameters $\mu, \Gamma$ depend on the categorical order of the language $L$:
\begin{equation*}
    \operatorname{d}\xi_{L,t} = \Gamma_{C(L)} \cdot (\xi_{L,t}-\mu_{C(L)}) \operatorname{d}t + \Lambda \operatorname{d}B_t
\end{equation*}
where $C(L)$ is the categorical order assigned to the language.

\paragraph{Labeling Categorical Basic Word Order}
This model requires labeling all nodes on the phylogenetic tree for categorical word orders.
We based the annotation on \citet{wals-81}.
In those languages where no annotation was available, we drew on \citep{gell-mann-origin-2011}, supplemented with language-specific literature.
In some cases, both `No dominant order' and one of SVO/SOV are plausible labels. In these cases, we opted for SOV/SVO, as we expect this to \emph{increase} the amount of variance that categorical order classes can explain, introducing at most a \emph{conservative} bias in estimating coadaptation.
We obtained the labels listed in Table~\ref{tab:categorical}.

\begin{table}
\begin{tabular}{llp{0.5\textwidth}lll}
\hline
	Language & Label & Rationale \\ \hline
\input{../analysis/categorical_order/output/additionalOrders.tex}
\hline
\end{tabular}
	\caption{Categorical labels for basic word order for languages with no annotation in \citet{wals-81}. This annotation is used for the control study in Section~\ref{sec:categorical}; it does not enter the main study.}\label{tab:categorical}
\end{table}
%For the phylogenetic analysis, we also annotated interior nodes for categorical word order, based on the literature.
%\begin{longtable}{llp{0.5\textwidth}lll}
%\hline
%Group & Label & Rationale \\ \hline
%\input{../analysis/categorical_order/output/additionalOrders_groups.tex}
%\hline
%\end{longtable}
%

\begin{figure}
    \centering
	\begin{tabular}{cc}
		\textbf{(A)} \\
    \includegraphics[width=0.5\textwidth]{../change/ornuhl-binom/categorical_order_gamma/fits/corr_sigma.pdf}
		\\
		\textbf{(B)} & \textbf{(C)} \\
		\includegraphics[width=0.5\textwidth]{../change/ornuhl-binom/categorical_order_gamma/fits/stationary_case_facet.pdf} &
    \includegraphics[width=0.4\textwidth]{../change/ornuhl-binom/categorical_order_gamma/fits/stationary_case_svo_sov.pdf}
	\end{tabular}
	\caption{Results of the phylogenetic analysis, controlling for conventional categorical word order labels. (A) Posterior over the correlation component $R_\Sigma$. Positive values indicate that grammar and usage evolve together. (B) Estimated stationary distributions for the four word order categories. (C) Estimated stationary distributions for the frequent classes, SOV and SVO.}
    \label{fig:langs-categorical}
\end{figure}


\paragraph{Results}
We show results in Figure~\ref{fig:langs-categorical}.
The posterior of $\Sigma$ (Figure~\ref{fig:langs-categorical} (A)) is very similar to the posterior in the main analysis; almost all of the posterior mass is on positive correlations between grammar and usage components ($R = 0.40$, 95\% credible interval [0.01, 0.71], $P(R<0) = 0.023$). % change/ornuhl-binom/case/separate_b/fits/correlation_sigma.txt
This shows that languages display coadaptation of grammar and usage even \emph{beyond} what is captured by conventional categorical word order labels.
Note that an association between grammar and usage on the level of categorical word order labels would already constitute evidence for coadaptation; this result shows that coadaptation happens at the more fine-grained level of subject-object position congruence.
In Figure~\ref{fig:langs-categorical} (B-C), we show the resulting stationary distributions for all four categorical word order types (B), and for the two frequent ones (SOV/SVO, (C)).
In the case of the frequent categories (SOV, SVO), the stationary distributions show a correlation in the two axes, illustrating that coadaptation happens even within these categorical classes.

\section{The Role of Case Marking}

Here, we test whether changes in the presence of case marking might mediate the coupling between word order and usage patterns.
We do this by fitting an extension of the model that can model different directions of change in languages with and without case marking, and checking whether the analysis continues to provide evidence for coevolution between word order and usage \emph{even beyond} what is captured by correlations of usage and word order with the presence of case marking.


\paragraph{Coding Languages for Case Marking}
We coded languages from our sample for the presence or absence of case marking on the basis of \citep{wals-49}, supplemented with information from the grammatical literature where no information was provided.
We amended the annotation from \citep{wals-49} to include only case marking that distinguishes between subjects and objects; this concerns several modern Celtic and Germanic languages, which have some nominal case marking but do not distinguish subjects and objects (e.g., Swedish and English use -\textit{s} to mark possessives, but do not distinguish nominal subjects and objects.).

We furthermore coded all interior nodes of the phylogenetic tree for case marking based on the linguistic literature.
In some cases, this annotation was unambiguous due to available historical documentation even though no treebank data was available (e.g., Proto-West-Scandinavian was a late form of Old Norse and had case markers).
In many other cases, cognate case markers are unambiguously attested both within and without a group, showing that they were present in the protolanguage (e.g., Proto-Germanic, Proto-Indo-Iranian).
Furthermore, in many protolanguages, case markers are commonly reconstructed based on their presence in different descendant branches (e.g., Proto-Indo-European, Proto-Afroasiatic, Proto-Common-Turkic, Proto-Uralic and Proto-Ugric). % Uraloc: Abondolo Uralic; Hajdu uralischen Sprachen; Marcantonio Uralic
Case is not unambiguously reconstructed for Proto-Niger-Congo; we verified that both possible parameter settings lead to qualitatively equivalent results (we report results under the assumption that it did not have case, with essentially indistinguishable results for the other cases).

\paragraph{Model of Change conditioned on Case Marking}
Based on the prior literature, we expect that languages without case marking will be biased towards low subject-object position congruence~\citep{vennemann1974explanation}.
To take this into account, we modified the model by conditioning the mean vector $\mu$ on the presence or absence of case in the language $L$.
\begin{equation*}
    \operatorname{d}\xi_{L,t} = \Gamma_{C(L)} \cdot (\xi_{L,t}-\mu_{C(L)}) \operatorname{d}t + \Lambda \operatorname{d}B_t
\end{equation*}
where $C(L)$ is $1$ if $L$ has case and $0$ else.
We set priors $\mu_{C(L)} \sim N(0,1)$ for both $C(L) = 0$, and $1$.

\paragraph{Results}
We plot the distribution of languages and the fitted stationary distributions, conditioned on $C(L)$, in Figure~\ref{fig:langs-case} (B-C).
In accordance with the prior literature, the model indicated that languages without case marking favor regions with low observed and optimized subject-object position congruence.
For languages with case marking, there was evidence for a bias towards higher subject-object position congruence.
We plot the posterior of the correlation component of $\Sigma$ (see Section~\ref{sec:instant-corr}) in Figure~\ref{fig:langs-case} (A).
The posterior probability that the correlation is $\leq 0$ is estimated to be $0.023$.
This shows that languages show coadaptation beyond usage and grammar in basic word order, even beyond an association with case marking.

\paragraph{Conclusion}
We found that, while case marking has a robust impact on subject-object position congruence, coadaptation continues to hold when controlling for this.

\begin{figure}
    \centering
	\begin{tabular}{cccc}
		\textbf{(A)} \\
    \includegraphics[width=0.5\textwidth]{../change/ornuhl-binom/case/separate_b/fits/corr_sigma.pdf} \\
		\textbf{(B)} & \textbf{(C)} \\
		\includegraphics[width=0.5\textwidth]{../analysis/figures/by_patient_marking.pdf} &
    \includegraphics[width=0.5\textwidth]{../change/ornuhl-binom/case/separate_b/fits/stationary_case.pdf}
	\end{tabular}
    \caption{Posterior of the correlation component $R_\Sigma$ (see Section~\ref{sec:instant-corr}) when accounting for case marking. Left: Languages by availability of morphological distinction between subject and object nouns. Right: Fitted stationary distribution, conditioned on case marking.}
    \label{fig:langs-case}
\end{figure}

%#Oliver A. Iggesen. 2013. Number of Cases.
%#In: Dryer, Matthew S. & Haspelmath, Martin (eds.)
%#The World Atlas of Language Structures Online.
%#Leipzig: Max Planck Institute for Evolutionary Anthropology.
%#(Available online at http://wals.info/chapter/49, Accessed on 2020-09-20.)


%\section{Pro-Drop}
%This correlation was not accounted for by differences in the availability of pro-drop (see SI Section X).


%Say something specific:
%Furthermore, we specifically considered what distinguishes the tree topologies of Old English from those of Modern English.
%- Old English -- English what distinguishes the tree structures?
%In this respect, Old English patterned more like contemporary Japanese.


%\subsection{Pro Drop}

%\subsection{Subject-Verb Agreement}

\section{Within-Language Correlates of Basic Word Order}

Here, we show that basic word order reflects optimization for DLM not only on the level of languages, but also on the level of individual sentences.

%\subsection{Coexpression: VS Order in SVO Languages}
In many SVO languages, certain intransitive subjects can appear after the verb (``along came a dog'').
This kind of ``intransitive inversion'' has been documented in many SVO languages, including English, Romance languages, and Chinese \citep[Chapter 17.2]{li1981mandarin}.
There are also languages whose basic word order is different in transitive and in intransitive clauses \citep{wals-82}; the World Atlas of Language Structures lists 13 languages with transitive SVO and intransitive VS basic word order \citep{wals-81,wals-82}, while it lists no languages with transitive VSO and intransitive SV order.
This observation has been formalized as the following language universal: \textit{If VS is dominant with transitives, it is also dominant with intransitives} (\citet[No 344]{plank2000the}, citing \citet{kozinsky1981Nekotorye}).
DLM provides an explanation for this universal.

%- literature
%- Unaccusative inversion
%-- Leonetti, Two Types of postervbal subject (Spanish): Unaccusative inversion (p. 17)
%-- English there inversion

We conjectured that, more generally, the rate of VS order is higher when no object is present than when an object is present.
For each language in our dataset, we collected statistics for all verbs with a subject and conducted the following logistic analysis:

\begin{equation}
\text{SV Order} \sim \text{Object is present}
\end{equation}

A positive effect indicates that presence of an object makes SV order more likely, compared to VS order.
Results are shown in Table S15.
As predicted, in most languages where there is variation between SV and VS order, a significant positive effect was observed.

\begin{longtable}{l|lllllll}
	\multicolumn{4}{p{0.8\textwidth}}{Table S15: Coefficients in logistic analysis regressing SV/VS Order based on the presence of an object. `SV Frequency' indicates the overall rate of SV order (as opposed to VS) in the language. A positive coefficient ($\beta > 0$) indicates that SV is more common in the presence of an object than when there is no object.}
	\\
		\multicolumn{4}{p{0.8\textwidth}}{}\\
Language & SV Frequency & $\beta$ & $p$ \\ \hline
\input{../collectPerVerbProperties/SV_withinLang/results_analyze_VSOrderWhenNoObject.R.tex}
\end{longtable}




%\subsection{Embedding: VSO in Embedded Clauses, SVO in Main Clauses}
In some predominant VSO languages, SVO is an alternative word order in unembedded clauses, whereas embedded clauses tend to only allow VSO.
This is in accordance with the predictions of DLM, which favors high subject-object position congruence in embedded clauses (see Figure 1B in the main paper).
Examples include relative clauses in Afroasiatic and Celtic (Standard Arabic~\citep{alqurashi:2012}, Breton \citep[][p. 80]{timm1988relative}, Ancient Egyptian \citep{gardiner1957egyptian}, Tuareg \citep[Chapter 12.1.2]{heath2005a}).
Conversely, in some SVO languages, embedded clauses show VSO order (Bantu, \citet{demuth1999verb}); Miza (Chadic) has SVO/VOS in main clauses and VOS in embedded clauses~\citep{wals-81}.
However, it is not generally true that VS order is more common in embedded clauses across all languages that have variation in basic word order.
For instance, German and Dutch can have VS in main clauses, but are almost always SV in subordinate clauses; the same holds for Quileute (Chimakuan)~\citep{wals-81}.




%\section{Detailed Diachronic Trajectories}
%-- coexpression
%-- position congruence
%-- case
%- English
%- French?
%- Ancient Greek?

\section{Estimating Usage and Grammar on Disjoint Corpora}


\begin{figure}
	\begin{center}
\includegraphics[width=0.6\textwidth]{../disjoint/plane-disjoint.pdf}
	\end{center}
	\caption{Estimating attested and optimized subject-object position congruence on disjoint datasets in English and Japanese. The blue datapoints are the results from the main experiments. For the other datapoints, we randomly split each of the corpora in half, and estimated attested and optimized position congruence separately on the two disjoint subsets.}\label{fig:disjoint}
\end{figure}

In the main experiment, we used the same corpora to estimate attested and optimized position congruence.
This is not circular, as optimized position congruence is estimated only on the basis of the tree topologies, without any influence from the actually observed orderings.
Here, we illustrate that equivalent results are obtained even when estimating both quantities on \emph{entirely disjoint} corpora.
Fot this, we randomly split the English and Japanese data into two equal-sized parts A and B.
In Figure~\ref{fig:disjoint}, we report the results obtained by (1) estimating optimized congruence on A and attested congruence on B (red), (2) the same with A and B reversed (blue), (3) and for comparison the result from the main experiment, where both quantities are estimated on the full dataset, i.e., the union of A and B.
The results show that qualitatively equivalent results are obtained in all three cases.

%\mhahn{TODO. Here, my plan is to illustrate that estimating quantities on disjoint corpora makes no difference, using just two or three languages as examples.}


\bibliography{literature}
\bibliographystyle{natbib}



\end{document}



\paragraph{Computing the Likelihood for Model with Changing Parameters}


\begin{equation}
\xi_{L,t'} | \xi_{L,t} \sim N\left(\mu' + e^{-\Delta \Gamma} (\xi_{L,t}-\mu'),\ \Omega - e^{-\Delta \Gamma} \Omega e^{-\Delta \Gamma^T}\right)
\end{equation}

Assume evolves according to $''$ parameters from $t'$ to $t''$ over time $\Delta''$.

\begin{align*}
\mathbb{E}[\xi_{L,t''} | \xi_{L,t}] &= \mathbb{E}\left[ \mathbb{E}[\xi_{L,t''} | \xi_{L,t'}] | \xi_{L,t}\right ] \\
&=  \mathbb{E}\left[  \mu'' + e^{-\Delta'' \Gamma''} (\xi_{L,t'}-\mu'')      | \xi_{L,t}\right ] \\
&= \mathbb{E}\left[  \mu'' | \xi_{L,t}\right ]  + \mathbb{E}\left[e^{-\Delta'' \Gamma''} (\xi_{L,t'}-\mu'')      | \xi_{L,t}\right ] \\
&= \mathbb{E}\left[  \mu'' | \xi_{L,t}\right ]  + e^{-\Delta'' \Gamma''} (\mathbb{E}\left[\xi_{L,t'}|\xi_{L,t}\right ]-\mu'')        \\
&= \mu'' + e^{-\Delta'' \Gamma''} ( \mu' + e^{-\Delta' \Gamma'} (\xi_{L,t}-\mu')    -\mu'') \\
&= \mu'' + e^{-\Delta'' \Gamma''} \mu' + e^{-\Delta'' \Gamma''} e^{-\Delta' \Gamma'} (\xi_{L,t}-\mu')    - e^{-\Delta'' \Gamma''} \mu'' \\
&= \mu'' + e^{-\Delta'' \Gamma''} \mu' + e^{-\Delta'' \Gamma''} e^{-\Delta' \Gamma'} \xi_{L,t}- e^{-\Delta'' \Gamma''} e^{-\Delta' \Gamma'}  \mu'    - e^{-\Delta'' \Gamma''} \mu'' \\
&= (1-e^{-\Delta'' \Gamma''}) \mu'' + e^{-\Delta'' \Gamma''} (1-e^{-\Delta' \Gamma'} ) \mu' + e^{-\Delta'' \Gamma''} e^{-\Delta' \Gamma'} \xi_{L,t}
\end{align*}

More generally:
\begin{align*}
    \mathbb{E}[\xi_{L,t_{K}} | \xi_{L,t_{0}}] = e^{-\Delta_K\Gamma_K} \dots e^{-\Delta_1\Gamma_1} \xi_{L,t_{0}} + \sum_{i=1}^K e^{-\Delta_K\Gamma_K} \dots e^{-\Delta_{i+1}\Gamma_{i+1}} (1-e^{-\Delta_{i}\Gamma_{i}}) \mu_i
\end{align*}


The cross-variance: Let $\xi_A$ be the last common ancestor.
\begin{align*}
    Cov(\xi_{1}, \xi_2) &= \mathbb{E}[ \mathbb{E}[\xi_1 \xi_2^T | \xi_A]] - \mathbb{E}[\xi_1] \mathbb{E}[\xi_1]^T \\
    &= \mathbb{E}[ \mathbb{E}[\xi_1| \xi_A] \mathbb{E}[\xi_2^T | \xi_A]] - \mathbb{E}[\xi_1] \mathbb{E}[\xi_1]^T  \\
    &= ... Var(\xi_A) ...
\end{align*}

The covariance, assuming this has developed from a draw $\xi_0$ from one of the stationary parts:
\begin{align*}
    \mathbb{E}[\xi_K \xi_K^T | \xi_0] &= \mathbb{E}\left[ \mathbb{E}[\xi_K \xi_K^T| \xi_{K-1}] | \xi_0\right] \\
    &=\mathbb{E}\left[  \Omega_K - e^{-\Delta_K \Gamma_K} \Omega_K e^{-\Delta_K \Gamma_K^T}  +  \mathbb{E}[\xi_K| \xi_{K-1}] \mathbb{E}[\xi_K^T| \xi_{K-1}]      | \xi_0\right] \\
    &=  \Omega_K - e^{-\Delta_K \Gamma_K} \Omega_K e^{-\Delta_K \Gamma_K^T}  +  \mathbb{E}\left[\mathbb{E}[\xi_K| \xi_{K-1}] \mathbb{E}[\xi_K^T| \xi_{K-1}]      | \xi_0\right] \\
    &=  \Omega_K - e^{-\Delta_K \Gamma_K} \Omega_K e^{-\Delta_K \Gamma_K^T}  +  \mathbb{E}\left[\mathbb{E}[\xi_K| \xi_{K-1}] \mathbb{E}[\xi_K^T| \xi_{K-1}]      | \xi_0\right] \\
    &=  \Omega_K - e^{-\Delta_K \Gamma_K} \Omega_K e^{-\Delta_K \Gamma_K^T}  + (1-e^{-\Delta_K \Gamma_K})\mu_K \mu_K^T (1-e^{-\Delta_K \Gamma_K})^T + e^{-\Delta_K\Gamma_K} \mathbb{E}\left[\xi_{K-1} \xi_{K-1}^T| \xi_0\right] e^{-\Delta_K\Gamma_K} \\
\end{align*}

So we have the recursive equation
\begin{equation}
V_K = \Omega_K - e^{-\Delta_K \Gamma_K} \Omega_K e^{-\Delta_K \Gamma_K^T}  + (1-e^{-\Delta_K \Gamma_K})\mu_K \mu_K^T (1-e^{-\Delta_K \Gamma_K})^T + e^{-\Delta_K\Gamma_K} V_{K-1} e^{-\Delta_K\Gamma_K}
\end{equation}
and 
\begin{align*}
\mathbb{E}[\xi_K \xi_K^T | \xi_0] & =  \sum_{i=1}^K e^{-\Delta_K\Gamma_K}\dots e^{-\Delta_{i+1}\Gamma_{i+1}} \Omega_K e^{-\Delta_{i+1}\Gamma_{i+1}}\dots e^{-\Delta_K\Gamma_K} \\
& - \sum_{i=1}^K e^{-\Delta_K\Gamma_K}\dots e^{-\Delta_{i}\Gamma_{i}} \Omega_K e^{-\Delta_{i}\Gamma_{i}}\dots e^{-\Delta_K\Gamma_K} \\
&+ \sum_{i=1}^K e^{-\Delta_K\Gamma_K}\dots e^{-\Delta_{i+1}\Gamma_{i+1}} (1-e^{-\Delta_i \Gamma_Ki})\mu_i \mu_i^T (1-e^{-\Delta_i \Gamma_i})^T e^{-\Delta_{i+1}\Gamma_{i+1}}\dots e^{-\Delta_K\Gamma_K} \\
\end{align*}






\begin{table}
\begin{tabular}{lllll}
Model & SDE & File & Log-Likelihood & loo \\ \hline
Uncorrelated   & $d\xi_t = \left(\begin{matrix} \sigma_1 & 0 \\ 0 & \sigma_2\end{matrix}\right) dW_t$ & 30.stan & -239\\
Correlated  & $d\xi_t = \left(\begin{matrix} \sigma_1 & \rho_1 \\ \rho_2 & \sigma_2\end{matrix}\right) dW_t$ & 27, 28, 29 & -197, -220, -195 \\
\hline
Correlated Drift & $d\xi_t = - \left(\begin{matrix} \gamma_1 & \gamma_{1,2} \\ \gamma_{2,1} & \gamma_{2,2}\end{matrix}\right) \left(\xi_t-\mu\right) dt + \left(\begin{matrix} \sigma_1 & 0 \\ 0 & \sigma_2\end{matrix}\right) dW_t$ & 28.stan & -297\\
Correlated + Drift & $d\xi_t = - \left(\begin{matrix} \gamma_1 & 0 \\ 0 & \gamma_{2,2}\end{matrix}\right) \left(\xi_t-\mu\right) dt + \left(\begin{matrix} \sigma_1 & \rho_1 \\ \rho_2 & \sigma_2\end{matrix}\right) dW_t$ & 26.stan & -303 \\
Uncorrelated + Drift & $d\xi_t = - \left(\begin{matrix} \gamma_1 & 0 \\ 0 & \gamma_{2,2}\end{matrix}\right) \left(\xi_t-\mu\right) dt + \left(\begin{matrix} \sigma_1 & 0 \\ 0 & \sigma_2\end{matrix}\right) dW_t$ & 25.stan & -343 \\
Correlated Drift & $d\xi_t = - \left(\begin{matrix} \gamma_1 & \gamma_{1,2} \\ \gamma_{2,1} & \gamma_{2,2}\end{matrix}\right) \left(\xi_t-\mu\right) dt + \left(\begin{matrix} \sigma_1 & \rho_1 \\ \rho_2 & \sigma_2\end{matrix}\right) dW_t$ & 27.stan & -297\\
\hline
Correlated  + Areas & $d\xi_t = - \left(\begin{matrix} \gamma_1 & 0 \\ 0 & \gamma_2\end{matrix}\right) \left(\xi_t-\mu(x)\right) dt + \left(\begin{matrix} \sigma_1 & \rho_1 \\ \rho_2 & \sigma_2\end{matrix}\right) dW_t$ & 29, 31, 32, 33 & -61, -69\\
Uncorrelated  + Areas & $d\xi_t = - \left(\begin{matrix} \gamma_1 & 0 \\ 0 & \gamma_2\end{matrix}\right) \left(\xi_t-\mu(x)\right) dt + \left(\begin{matrix} \sigma_1 & 0 \\ 0 & \sigma_2\end{matrix}\right) dW_t$ & 30 & 113\\
\hline
Correlated  + Area(t) & $d\xi_t = - \left(\begin{matrix} \gamma_1 & 0 \\ 0 & \gamma_2\end{matrix}\right) \left(\xi_t-\mu(x)\right) dt + \left(\begin{matrix} \sigma_1 & \rho_1 \\ \rho_2 & \sigma_2\end{matrix}\right) dW_t$ & 35, 36, 38 & -47, -72, -54\\
Uncorrelated  + Area(t) & $d\xi_t = - \left(\begin{matrix} \gamma_1 & 0 \\ 0 & \gamma_2\end{matrix}\right) \left(\xi_t-\mu(x)\right) dt + \left(\begin{matrix} \sigma_1 & 0 \\ 0 & \sigma_2\end{matrix}\right) dW_t$ & 37 & -109\\
\hline
\end{tabular}
\caption{Phylogenetic drift models. For each model, we provide a representation as a stochastic differential equation, and the logarithm of the estimated marginal likelihood.}
\end{table}

\begin{table}[]
    \centering
    \begin{tabular}{c|c}
   Parameter & Prior \\ 
  $\left(\begin{matrix} \sigma_1 & 0 \\ 0 & \sigma_2\end{matrix}\right)$       &  \\
         $\left(\begin{matrix} \sigma_1 & \rho_1 \\ \rho_2 & \sigma_2\end{matrix}\right)$ & 
    \end{tabular}
    \caption{Caption}
    \label{tab:my_label}
\end{table}
